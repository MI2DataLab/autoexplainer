\documentclass{article}%
\usepackage[T1]{fontenc}%
\usepackage[utf8]{inputenc}%
\usepackage{lmodern}%
\usepackage{textcomp}%
\usepackage{lastpage}%
\usepackage{geometry}%
\geometry{tmargin=2cm,lmargin=2cm}%
\usepackage{hyperref}%
\usepackage{booktabs}%
\usepackage{graphicx}%
%
\title{AutoeXplainer Report}%
\date{}%
%
\begin{document}%
\normalsize%
\maketitle%
\section*{General information}%
\label{sec:Generalinformation}%
\textbf{Model name: }%
\textit{DenseNet121 \newline%
}%
\textbf{Dataset name: }%
\textit{Imagenette \newline%
}%
\textbf{Execution time: }%
\textit{3088.521 s \newline%
}%
\textbf{Package version: }%
\textit{0.0.3 \newline%
}%
\textbf{Date: }%
\textit{2023{-}01{-}26 \newline%
}%
\textbf{Selected method: }%
\textit{GradCam \newline%
}%
\textbf{Number of images: }%
\textit{2}

%
\section*{Model performance}%
\label{sec:Modelperformance}%
\textbf{Accuracy: }%
\textit{1.0 \newline%
}%
\textbf{F1 macro: }%
\textit{1.0 \newline%
}%
\textbf{Balanced accuracy: }%
\textit{1.0 \newline%
}

%
\section*{Table of results}%
\label{sec:Tableofresults}%
\begin{footnotesize}%
\begin{flushleft} %
\begin{tabular}{lrrrrrrr}
\toprule
    Explanation Name &  Rank &  Faithfulness Est. ↑ &  Avg Sensitivity ↓ &  IROF ↑ &  Sparseness ↑ &  Time elapsed [s] &  Agg. Score \\
\midrule
             GradCam &     1 &                0.500 &              0.014 &  45.814 &         0.560 &             0.749 &          10 \\
            Saliency &     2 &                0.390 &              0.055 &  48.937 &         0.556 &             1.919 &           7 \\
Integrated Gradients &     3 &               -0.142 &              0.032 &  21.450 &         0.681 &            31.562 &           5 \\
          KernelSHAP &     4 &                0.228 &              0.318 &  30.064 &         0.414 &            31.819 &           2 \\
\bottomrule
\end{tabular}
%
\end{flushleft}%
\end{footnotesize}%
\textbf{Table description \newline%
}%
Arrow next to the metric names indicates whether larger or smaller values of metric are better. Time elapsed shows time that was required for computation of attribution for given batch of images. When there is a tie in Aggregated Score, the best metric is chosen based on computation time.

%
\newpage%
\section*{Details}%
\label{sec:Details}%
\subsection*{Explanations:}%
\label{subsec:Explanations}%
\begin{itemize}%
\item%
\textbf{KernelSHAP}%
: Uses the LIME framework to approximate Shapley values from game theory.%
\href{https://arxiv.org/abs/1705.07874}{(Lundberg and Su{-}In Lee, 2017)}%
\newline%
%
Explanation's parameters: \newline%
%
\texttt{%
\{   'explanation\_parameters': \{   'baseline\_function': baseline\_color\_black,\newline%
                                  'baseline\_function\_name': 'black',\newline%
                                  'n\_samples': 50\},\newline%
    'mask\_parameters': \{'n\_segments': 50\}\} \newline%
%
}%
\item%
\textbf{Integrated Gradients}%
: Approximates feature importances by computing gradients for model outputs for images from the straight line between the original image and the baseline black image. Later, for each feature, the integral is approximated using these gradients.%
\href{https://arxiv.org/abs/1703.01365}{(Sundararajan et al., 2017)}%
\newline%
%
Explanation's parameters: \newline%
%
\texttt{%
\{   'explanation\_parameters': \{   'baseline\_function': baseline\_color\_black,\newline%
                                  'baseline\_function\_name': 'black',\newline%
                                  'n\_steps': 20\}\} \newline%
%
}%
\item%
\textbf{GradCam}%
:  For the selected layer and a target class, it computes gradients, multiplies its average by layer activations and returns only the positive part of the result. For images with more than one channel, it returns the positive part of the sum of results from all channels.%
\href{https://arxiv.org/abs/1610.02391}{(Selvaraju et al., 2016)}%
\newline%
%
Explanation's parameters: \newline%
%
\texttt{%
\{   'explanation\_parameters': \{   'relu\_attributions': True,\newline%
                                  'selected\_layer': 'features.denseblock4.denselayer16.conv2'\}\} \newline%
%
}%
\item%
\textbf{Saliency}%
:  Is based on computing gradients. The idea is to approximate CNN's output for a given class in the neighborhood of the image using a linear approximation and interpret the coefficients vector as an importance vector for all pixels.%
\href{https://arxiv.org/abs/1312.6034}{(Simonyan et al., 2013)}%
\newline%
%
Explanation's parameters: \newline%
%
\texttt{%
\{'explanation\_parameters': \{'abs': True\}\} \newline%
%
}%
\end{itemize}

%
\newpage%
\subsection*{Metrics:}%
\label{subsec:Metrics}%
\begin{itemize}%
\item%
\textbf{Faithfulness Estimate}%
: Evaluates the relevance of the computed explanation by calculating the correlation between computed feature attribution and probability drops after removing features.%
\href{https://arxiv.org/abs/1806.07538}{(Alvarez{-}Melis et al., 2018)}%
\newline%
%
Metric's parameters: \newline%
%
\texttt{%
\{   'call': \{\},\newline%
    'init': \{   'disable\_warnings': True,\newline%
                'display\_progressbar': False,\newline%
                'features\_in\_step': 256,\newline%
                'normalise': True,\newline%
                'perturb\_baseline': 'black',\newline%
                'softmax': True\}\} \newline%
%
}%
\item%
\textbf{Average Sensitivity}%
: A metric that measures an average of how sensitive to perturbations the explanation method is. The implementation uses a Monte Carlo sampling{-}based approximation.%
\href{https://arxiv.org/abs/1901.09392}{(Yeh et al., 2019)}%
\newline%
%
Metric's parameters: \newline%
%
\texttt{%
\{   'call': \{\},\newline%
    'init': \{   'disable\_warnings': True,\newline%
                'display\_progressbar': False,\newline%
                'lower\_bound': 0.2,\newline%
                'norm\_denominator': fro\_norm,\newline%
                'norm\_numerator': fro\_norm,\newline%
                'normalise': True,\newline%
                'nr\_samples': 20,\newline%
                'perturb\_func': uniform\_noise,\newline%
                'perturb\_radius': 0.2,\newline%
                'similarity\_func': difference\}\} \newline%
%
}%
\item%
\textbf{Iterative Removal of Features}%
: Iteratively removes the most important features and measures the change in probability in the model prediction for a given class. It plots the probability for a given class with respect to the number of removed features and computes the area over the curve.%
\href{https://arxiv.org/abs/2003.08747}{(Rieger at el., 2020)}%
\newline%
%
Metric's parameters: \newline%
%
\texttt{%
\{   'call': \{\},\newline%
    'init': \{   'disable\_warnings': True,\newline%
                'display\_progressbar': False,\newline%
                'perturb\_baseline': 'mean',\newline%
                'return\_aggregate': False,\newline%
                'segmentation\_method': 'slic',\newline%
                'softmax': True\}\} \newline%
%
}%
\item%
\textbf{Sparseness}%
: With the use of the Gini Index measures how imbalanced feature importances given by the explanation method are.%
\href{https://arxiv.org/abs/1810.06583}{(Chalasani et al., 2020)}%
\newline%
%
Metric's parameters: \newline%
%
\texttt{%
\{'call': \{\}, 'init': \{'disable\_warnings': True, 'display\_progressbar': False\}\} \newline%
%
}%
\end{itemize}

%
\subsection*{Aggregation parameters}%
\label{subsec:Aggregationparameters}%
\texttt{%
\{   'first\_stage\_aggregation\_function': 'mean',\newline%
    'second\_stage\_aggregation\_function': 'rank\_based',\newline%
    'second\_stage\_aggregation\_function\_aggregation\_parameters': \{\}\}%
}

%
\newpage%
\section*{Examples of explanations}%
\label{sec:Examplesofexplanations}%


\begin{figure}[!h]%
\centering%
\includegraphics[width=17cm]{C:/Users/matem/AppData/Local/Temp/pylatex-tmp.r449jxkq/ac19d061-27a7-4844-9fa0-e63ceb1e2aba.pdf}%
\end{figure}

%
\end{document}