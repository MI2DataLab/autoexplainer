{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AutoExplainer","text":"<p>A python package for automated selection of explanation method for CNNs.</p>"},{"location":"#installation","title":"Installation","text":"<p>Prerequisites: installed python 3.9.</p>"},{"location":"#with-poetry","title":"With <code>poetry</code>","text":"<p>To install all dependencies from <code>poetry.toml</code> file using <code>poetry</code> run:</p> <pre><code>git clone https://github.com/MI2DataLab/autoexplainer.git\ncd autoexplainer\npoetry config virtualenvs.in-project true\npoetry shell # if you want create dedicated .venv inside autoexplainer\npoetry install\n</code></pre> <p>To use created enviroment, activate it with <code>poetry shell</code>.</p>"},{"location":"#with-pip","title":"With <code>pip</code>","text":"<p>To install dependencies the regular way you can use <code>pip</code>:</p> <pre><code>git clone https://github.com/MI2DataLab/autoexplainer.git\ncd autoexplainer\npip install -r requirements.txt\n</code></pre>"},{"location":"#to-update-the-environment","title":"To update the environment","text":"<p>After pulled changes in dependencies, you can update dependencies with:</p> <pre><code>poetry update\n</code></pre>"},{"location":"#installing-torch-121-with-cuda-116-support","title":"Installing torch 12.1 with CUDA 11.6 support","text":"<p>To uninstall current torch version and install torch 12.1 with CUDA 11.6 support, run:</p> <pre><code>pip uninstall torch torchvision\npip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n</code></pre>"},{"location":"#to-create-pdf-report-with-function-to_pdf","title":"To create PDF report with function <code>to_pdf</code>","text":"<p>The function <code>to_pdf</code> creates both <code>.tex</code> and <code>.pdf</code> report versions. Due to this fact, additional features have to be installed to render PDF report properly: * install LaTeX eg. MiKTeX and add to PATH   * in MiKTeX enable automatically installing missing packages * add dependency with <code>pip</code>:</p> <pre><code>pip install pylatex\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>See sample notebook in <code>development/notebooks/auto_explainer_usage.ipynb</code>.</p>"},{"location":"#selection-process","title":"Selection process","text":"<p>The most time consuming part. First, all methods are evaluated on provided data. Then, the best method is selected by aggregating raw results. Finally, a report is generated.</p> <pre><code>from autoexplainer import AutoExplainer\n\nauto_explainer = AutoExplainer(model, data, targets)\n\n# compute all metric values and see not aggregated results (very long)\nauto_explainer.evaluate()\nauto_explainer.raw_results\n\n# aggregate metric scores and see aggregated results (almost instant)\nauto_explainer.aggregate()\nauto_explainer.first_aggregation_results  # single value per (method, metric) pair\nauto_explainer.second_aggregation_results  # single value per method\n\n# produce a pdf report\nauto_explainer.to_html('examples/example_report.html')\n</code></pre>"},{"location":"#using-results","title":"Using results","text":"<p>Later, the selected explanation method can be extracted and used right away to explain more data.</p> <pre><code>best_explanation = auto_explainer.get_best_explanation()\nnew_attributions = best_explanation.explain(model, data, targets)\n</code></pre> <p>This <code>best_explanation</code> object contains all the information about the selected method, including the name of the method, the parameters used, and the attributions of data used during explanation method selection.</p> <pre><code>best_explanation.name\nbest_explanation.parameters\nbest_explanation.attributions\n</code></pre> <p>It is also possible to calculate metric values for methods used during selection process but on other data. Values can be either raw (1 value per data point) or aggregated (1 value only, as in <code>auto_explainer.second_aggregation_results</code>).</p> <pre><code>raw_metric_scores = best_explanation.evaluate(model, data, targets, new_attributions)\naggregated_metric_scores = best_explanation.evaluate(model, data, targets, attributions, aggregate=True)\n\n</code></pre>"},{"location":"#development","title":"Development","text":""},{"location":"#running-tests","title":"Running tests","text":"<p>To run test (<code>-n</code> tests in parallel):</p> <pre><code>pytest tests -n auto\n</code></pre> <p>or 1 test at a time:</p> <pre><code>pytest tests\n</code></pre> <p>or a selected test a time:</p> <pre><code>pytest tests/test_autoexplainer.py\n</code></pre> <p>or print output during tests:</p> <pre><code>pytest -s tests\n</code></pre>"},{"location":"#running-pre-commit-hooks","title":"Running <code>pre-commit</code> hooks","text":"<p>To check formatting, linting, and other checks before commiting, run:</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"#generating-documentation","title":"Generating documentation","text":"<p>To generate documentation, run:</p> <pre><code>mkdocs build\n</code></pre> <p>The documentation will be generated in <code>site/</code> directory.</p> <p>To generate documentation and serve it locally, run:</p> <pre><code>mkdocs serve\n</code></pre> <p>The documentation will be available at <code>http://127.0.0.1:8000/</code>.</p> <p>If You didn't activate poetry shell, precede commands above with <code>poetry run</code>.</p>"},{"location":"#authors","title":"Authors","text":"<p>This repository contains all code used for our bachelor thesis written at the Faculty of Mathematics and Information Science, Warsaw University of Technology.</p> <p>This project was generated using the wolt-python-package-cookiecutter template.</p>"},{"location":"api_docs/","title":"API documentation","text":""},{"location":"api_docs/#autoexplainer","title":"AutoeXplainer","text":""},{"location":"api_docs/#autoexplainer.autoexplainer.AutoExplainer","title":"<code>AutoExplainer</code>","text":"<p>The main class that evaluates a series of explanation methods and chooses the best one.</p> <p>Attributes:</p> Name Type Description <code>raw_results</code> <code>Dict</code> <p>Raw values of metrics computed for each observation for each explanation.</p> <code>first_aggregation_results</code> <code>Dict</code> <p>Values of metrics aggregated across observations, i.e. each explanation function has value for each metric.</p> <code>second_aggregation_results</code> <code>Dict</code> <p>Values of metrics aggregated for each explanation method. Each explanation method has single value, that represents overall quality.</p> <code>best_explanation_name</code> <code>str</code> <p>Name of the selected best explanation found.</p> Source code in <code>autoexplainer/autoexplainer.py</code> <pre><code>class AutoExplainer:\n\"\"\"\n    The main class that evaluates a series of explanation methods and chooses the best one.\n    Attributes:\n        raw_results (Dict): Raw values of metrics computed for each observation for each explanation.\n        first_aggregation_results (Dict): Values of metrics aggregated across observations, i.e. each explanation\n            function has value for each metric.\n        second_aggregation_results (Dict): Values of metrics aggregated for each explanation method. Each explanation\n            method has single value, that represents overall quality.\n        best_explanation_name (str): Name of the selected best explanation found.\n    \"\"\"\n\n    KNOWN_EXPLANATION_HANDLERS: Dict = {\n        \"kernel_shap\": KernelShapHandler,\n        \"integrated_gradients\": IntegratedGradients,\n        \"grad_cam\": GradCamHandler,\n        \"saliency\": SaliencyHandler,\n    }\n    KNOWN_METRIC_HANDLERS: Dict = {\n        \"faithfulness_estimate\": FaithfulnessEstimateHandler,\n        \"average_sensitivity\": AvgSensitivityHandler,\n        \"irof\": IROFHandler,\n        \"sparseness\": SparsenessHandler,\n    }\n    KNOWN_FIRST_STAGE_AGGREGATION_FUNCTIONS: Dict = {\n        \"mean\": first_stage_aggregation_mean,\n        \"median\": first_stage_aggregation_median,\n        \"max\": first_stage_aggregation_max,\n        \"min\": first_stage_aggregation_min,\n    }\n    KNOWN_SECOND_STAGE_AGGREGATION_FUNCTIONS: Dict = {\n        \"rank_based\": second_stage_aggregation_rank_based,\n        \"weighted_mean\": second_stage_aggregation_weighted_mean,\n    }\n\n    def __init__(\n        self, model: torch.nn.Module, data: torch.Tensor, targets: torch.Tensor, device: str = \"cpu\", seed: int = 42\n    ):\n\"\"\"\n\n        Args:\n            model (torch.nn.Module): Convolutional neural network to be explained. On this model some explanation and metric\n                parameters will be inferred.\n            data (torch.Tensor): Data that will be used for explanation method evaluation. shape: (N, C, H, W).\n            targets (torch.Tensor): Labels for provided data. Encoded as integer vector with shape (N,).\n        \"\"\"\n        self._check_model(model)\n        self._check_data(data, targets)\n        self._second_stage_aggregation_function_name = None\n        self._first_stage_aggregation_function_name = None\n\n        self.explanation_handlers: Dict = None\n        self.metric_handlers: Dict = None\n        self.first_aggregation_results: Dict = None\n        self.second_aggregation_results: Dict = None\n        self.best_explanation_name: str = None\n        self.aggregation_parameters: Dict = None\n\n        model = model.to(device)\n        model.eval()\n        self.model = fix_relus_in_model(model)\n        self.data = data.to(device)\n        self.targets = targets.to(device)\n        self.raw_results: Dict = {}\n        self.times_methods: Dict = {}\n        self.times_metrics: Dict = {}\n        self.times_metrics_aggregated: Dict = {}\n        self.times_total = 0.0\n        self.device = device\n        self._set_seed(seed)\n\n    def evaluate(\n        self,\n        explanations: List[str] = None,\n        metrics: List[str] = None,\n        explanation_params: Dict = None,\n        metrics_params: Dict = None,\n    ) -&gt; None:\n\"\"\"\n        Evaluates explanation methods. Stores results in ``.raw_results`` attribute.\n        Args:\n            explanations (List[str]): List of names of explanation methods to be evaluated.\n                                      By default, uses all available explanation methods.\n                                      Accepts lists with subset of: ``{\"saliency\", \"grad_cam\", \"integrated_gradients\", \"kernel_shap\"}``.\n            metrics (List[str]): List of names of evaluation metrics to be used. By default, uses all available metrics.\n                                Accepts lists with subset of: ``{\"irof\", \"sparseness\", \"average_sensitivity\", \"faithfulness_estimate\"}``.\n            explanation_params (Dict[str, Dict]): Allows to override default parameters of selected explanation functions.\n                                                Accept Dictionary with form ``{\"explanation_name\": &lt;Dictionary with parameters&gt;}``.\n                                                See corresponding ExplanationHandler to see what parameters are accepted.\n            metrics_params (Dict[str, Dict]): Allows to override default parameters of selected metrics.\n                                              Accept Dictionary with form ``{\"metric_name\": &lt;Dictionary with parameters&gt;}``.\n                                              See corresponding MetricHandler to see what parameters are accepted.\n\n        \"\"\"\n        self._check_method_and_metric_names(explanations, metrics)\n        if explanations is None:\n            explanations = self.KNOWN_EXPLANATION_HANDLERS\n        if metrics is None:\n            metrics = self.KNOWN_METRIC_HANDLERS\n        self._check_method_and_metric_params_dicts(explanations, metrics, explanation_params, metrics_params)\n        if explanation_params is None:\n            explanation_params = {}\n        if metrics_params is None:\n            metrics_params = {}\n\n        print(\"\\nPreparing explanation methods and metric handlers...\\n\")\n\n        self.explanation_handlers = {\n            explanation_name: self.KNOWN_EXPLANATION_HANDLERS[explanation_name](\n                self.model, self.data, self.targets, explanation_params.get(explanation_name)\n            )\n            for explanation_name in explanations\n        }\n        self.metric_handlers = {\n            metric_name: self.KNOWN_METRIC_HANDLERS[metric_name](\n                self.model, self.data, self.targets, metrics_params.get(metric_name)\n            )\n            for metric_name in metrics\n        }\n        self.times_metrics = {metric_name: {} for metric_name in metrics}\n\n        print(\"\\tNumber of explanation methods to evaluate: \", len(self.explanation_handlers))\n        print(\n            \"\\tExplanation methods selected: \"\n            + f\"{', '.join([EXPLANATION_NAME_SHORT_TO_LONG[x] for x in list(self.explanation_handlers.keys())])}\"\n        )\n        print(\"\")\n        print(\"\\tNumber of metrics used during evaluation: \", len(self.metric_handlers))\n        print(\n            \"\\tMetrics selected: \"\n            + f\"{', '.join([METRIC_NAME_SHORT_TO_LONG[x] for x in list(self.metric_handlers.keys())])}\"\n        )\n\n        pbar = tqdm.tqdm(self.explanation_handlers.items(), desc=\"Creating attributions\")\n        for explanation_name, explanation_handler in pbar:\n            start_time = time.time()\n            pbar.set_description(f\"Creating attributions for {explanation_name}\")\n            explanation_handler.explain(model=self.model, data=self.data, targets=self.targets)\n            self.times_methods[explanation_name] = round(time.time() - start_time, 3)\n\n        for explanation_name in self.explanation_handlers.keys():\n            self.raw_results[explanation_name] = {}\n\n        print(\"Creating attribution finished. Starting evaluation.\")\n        print(\"Evaluation may take a very long time, please be patient...\")\n\n        pbar = tqdm.tqdm(\n            itertools.product(self.metric_handlers.items(), self.explanation_handlers.items()),\n            total=len(self.metric_handlers) * len(self.explanation_handlers),\n            desc=\"Evaluating metrics\",\n        )\n        for (metric_name, metric_handler), (explanation_name, explanation_handler) in pbar:\n            start_time = time.time()\n            pbar.set_description(f\"Evaluating: method {explanation_name} and metric {metric_name}\")\n            self.raw_results[explanation_name][metric_name] = metric_handler.compute_metric_values(\n                model=self.model,\n                data=self.data,\n                targets=self.targets,\n                attributions=explanation_handler.attributions.to(next(self.model.parameters()).device),\n                explanation_func=explanation_handler.get_explanation_function(),\n            )\n            self.times_metrics[metric_name][explanation_name] = round(\n                time.time() - start_time + self.times_methods[explanation_name], 3\n            )\n\n        self.times_metrics_aggregated = {\n            metric_name: round(sum(self.times_metrics[metric_name].values()), 3)\n            for metric_name in self.times_metrics.keys()\n        }\n        self.times_total = round(sum(self.times_metrics_aggregated.values()), 3)\n\n        print(f\"Evaluating metrics finished after {self.times_total} seconds.\")\n\n    def aggregate(\n        self,\n        first_stage_aggregation_function_name: str = \"mean\",\n        second_stage_aggregation_function_name: str = \"rank_based\",\n        second_stage_aggregation_function_aggregation_parameters: Dict = None,\n    ) -&gt; None:\n\"\"\"\n        Aggregates raw result computed in .evaluate() method in two steps. First, aggregates metric scores across\n        provided observations, i.e. each explanation method has a  value for each metric. Secondly, aggregates\n        scores across available metrics, i.e. each explanation method has a single value that represents overall quality.\n\n        Stores both aggregation steps in the attributes ``first_aggregation_results`` and ``second_aggregation_results``.\n\n        Args:\n            first_stage_aggregation_function_name ({\"mean\", \"median\", \"min\",\"max\"}): Name of the function for the first stage aggregation.\n            second_stage_aggregation_function_name ({\"mean\", \"median\", \"min\",\"max\"}): Name of the function for second stage aggregaton.\n            second_stage_aggregation_function_aggregation_parameters (Dict): Parameters for the second stage aggregation function.\n\n        \"\"\"\n\n        self._check_is_after_evaluation()\n        self._check_aggregation_parameters(\n            first_stage_aggregation_function_name,\n            second_stage_aggregation_function_name,\n            second_stage_aggregation_function_aggregation_parameters,\n        )\n        self._first_stage_aggregation_function_name = first_stage_aggregation_function_name\n        self._second_stage_aggregation_function_name = second_stage_aggregation_function_name\n\n        if second_stage_aggregation_function_aggregation_parameters is None:\n            second_stage_aggregation_function_aggregation_parameters = {}\n        self.first_aggregation_results = self.KNOWN_FIRST_STAGE_AGGREGATION_FUNCTIONS[\n            first_stage_aggregation_function_name\n        ](self.raw_results)\n        self.second_aggregation_results = self.KNOWN_SECOND_STAGE_AGGREGATION_FUNCTIONS[\n            second_stage_aggregation_function_name\n        ](self.first_aggregation_results, second_stage_aggregation_function_aggregation_parameters)\n        sorted_results = sorted(self.second_aggregation_results.items(), key=lambda x: x[1], reverse=True)\n        if len(sorted_results) &gt; 1:\n            best_result, second_best_result = sorted_results[0], sorted_results[1]\n            if best_result[1] == second_best_result[1]:\n                if self.times_methods[best_result[0]] &gt; self.times_methods[second_best_result[0]]:\n                    best_result = second_best_result\n            self.best_explanation_name = best_result[0]\n        else:\n            self.best_explanation_name = sorted_results[0][0]\n\n        self.aggregation_parameters = {\n            \"first_stage_aggregation_function\": self.KNOWN_FIRST_STAGE_AGGREGATION_FUNCTIONS[\n                first_stage_aggregation_function_name\n            ],\n            \"second_stage_aggregation_function\": self.KNOWN_SECOND_STAGE_AGGREGATION_FUNCTIONS[\n                second_stage_aggregation_function_name\n            ],\n            \"second_stage_aggregation_function_aggregation_parameters\": second_stage_aggregation_function_aggregation_parameters,\n        }\n\n    def to_html(\n        self, file_path: str, model_name: str = None, dataset_name: str = None, labels: Dict[int, str] = None\n    ) -&gt; None:\n\"\"\"\n        Generates evaluation report as HTML file.\n        Args:\n            file_path (str): Target file path.\n            model_name (str): Name of model to show inside report.\n            dataset_name (str): Name of dataset to show inside report.\n            labels (Dict[int,str]): Mapping between class number and class names. e.g. ``{0:\"dog\", 1:\"cat\", 2:\"fish\"}`` for labels\n                                    inside report.\n        \"\"\"\n        assert self.first_aggregation_results is not None, \"Aggregated results are needed for report generation.\"\n        assert self.second_aggregation_results is not None, \"Aggregated results are needed for report generation.\"\n\n        environment = Environment(loader=PackageLoader(\"autoexplainer\"))\n        template = environment.get_template(\"report.html\")\n\n        report_info = self._get_info_for_report(labels=labels)\n\n        pic_io_bytes = io.BytesIO()\n        fig = report_info[\"fig_with_examples\"]\n        fig.savefig(pic_io_bytes, format=\"png\")\n        pic_io_bytes.seek(0)\n        pic_hash = base64.b64encode(pic_io_bytes.read())\n\n        _, _, *float_columns, _ = report_info[\"result_dataframe\"].columns\n\n        html_table = (\n            report_info[\"result_dataframe\"]\n            .style.set_properties(\n                subset=[\"Agg. Score\", \"Explanation Name\"], **{\"font-weight\": \"bold\", \"text-align\": \"center\"}\n            )\n            .set_properties(border=0)\n            .hide_index()\n            .format(\"{:.3f}\", subset=float_columns)\n            .render()\n        )\n\n        rendered = template.render(\n            model_name=model_name,\n            dataset_name=dataset_name,\n            dataframe_html=html_table,\n            pic_hash=pic_hash.decode(),\n            **report_info,\n        )\n        with open(file_path, mode=\"w\", encoding=\"utf-8\") as results:\n            results.write(rendered)\n\n    def to_pdf(\n        self,\n        folder_path: str = \"\",\n        model_name: str = \"name of the model\",\n        dataset_name: str = \"name of the dataset\",\n        labels: Dict[int, str] = None,\n    ) -&gt; None:\n\n\"\"\"\n        Creates PDF report from dict stored in the attribute ``first_aggregation_results``.\n        Needs Latex packages installed to run - see README.\n\n        Args:\n            folder_path (str): Path to directory, where the reports (PDF and tex) should be created.\n            model_name (str): Name of model to show inside report.\n            dataset_name (str): Name of dataset to show inside report.\n            labels (Dict[int,str]): Mapping between class number and class names. e.g. ``{0:\"dog\", 1:\"cat\", 2:\"fish\"}`` for labels\n                                    inside report.\n\n        \"\"\"\n        self._check_is_after_aggregation()\n\n        tex_file = os.path.join(folder_path, \"report.tex\")\n        pdf_file = os.path.join(folder_path, \"report.pdf\")\n\n        if os.path.exists(tex_file):\n            os.remove(tex_file)\n        if os.path.exists(pdf_file):\n            os.remove(pdf_file)\n\n        report_info = self._get_info_for_report(labels=labels)\n\n        left_margin = 2\n        max_nr_columns_in_table = 5\n        geometry_options = {\"tmargin\": \"2cm\", \"lmargin\": f\"{left_margin}cm\"}\n        doc = Document(geometry_options=geometry_options)\n        doc.preamble.append(Command(\"title\", \"AutoeXplainer Report\"))\n        doc.preamble.append(Command(\"date\", \"\"))\n        doc.packages.append(Package(\"hyperref\"))\n        doc.packages.append(Package(\"booktabs\"))\n        doc.append(NoEscape(r\"\\maketitle\"))\n\n        results = report_info[\"result_dataframe\"]\n\n        metric_name_copy = copy.deepcopy(METRIC_NAME_SHORT_TO_LONG)\n        metric_name_copy[\"explanation_name\"] = \"explanation name\"\n        metric_name_copy[\"Rank\"] = \"Rank\"\n        metric_name_copy[\"Agg. Score\"] = \"Agg. Score\"\n        explanation_methods = report_info[\"methods\"]\n        metrics = report_info[\"metrics\"]\n        metrics_used = copy.deepcopy(metrics)\n\n        metrics = [\"explanation name\", \"Rank\"] + metrics + [\"Agg. Score\"]\n        data = copy.deepcopy(results)\n\n        def hyperlink(url: str, text: str) -&gt; NoEscape:  # type: ignore\n            return NoEscape(r\"\\href{\" + url + \"}{\" + escape_latex(text) + \"}\")\n\n        # create content of  the Document\n        with doc.create(Section(\"General information\", numbering=False)):\n            doc.append(bold(\"Model name: \"))\n            doc.append(italic(f\"{model_name} \\n\"))\n            doc.append(bold(\"Dataset name: \"))\n            doc.append(italic(f\"{dataset_name} \\n\"))\n            doc.append(bold(\"Execution time: \"))\n            doc.append(italic(f\"{report_info['execution_time']} s \\n\"))\n            doc.append(bold(\"Package version: \"))\n            doc.append(italic(f\"{report_info['autoexplainer_version']} \\n\"))\n            doc.append(bold(\"Date: \"))\n            doc.append(italic(f\"{report_info['date']} \\n\"))\n            doc.append(bold(\"Selected method: \"))\n            doc.append(italic(f\"{report_info['selected_method']} \\n\"))\n            doc.append(bold(\"Number of images: \"))\n            doc.append(italic(f\"{report_info['n_images']}\"))\n\n        with doc.create(Section(\"Model performance\", numbering=False)):\n            doc.append(bold(\"Accuracy: \"))\n            doc.append(italic(f\"{report_info['model_acc']} \\n\"))\n            doc.append(bold(\"F1 macro: \"))\n            doc.append(italic(f\"{report_info['model_f1_macro']} \\n\"))\n            doc.append(bold(\"Balanced accuracy: \"))\n            doc.append(italic(f\"{report_info['model_bac']} \\n\"))\n\n        with doc.create(Section(\"Table of results\", numbering=False)):\n            doc.append(NoEscape(r\"\\begin{footnotesize}\"))\n            doc.append(NoEscape(r\"\\begin{flushleft} \"))\n            doc.append(NoEscape(report_info[\"result_dataframe\"].to_latex(index=False)))\n            doc.append(NoEscape(r\"\\end{flushleft}\"))\n            doc.append(NoEscape(r\"\\end{footnotesize}\"))\n            doc.append(bold(\"Table description \\n\"))\n            doc.append(\n                \"Arrow next to the metric names indicates whether larger or smaller values of metric are better. Time elapsed shows time that was required for computation of attribution for given batch of images. When there is a tie in Aggregated Score, the best metric is chosen based on computation time.\"\n            )\n\n        doc.append(NewPage())\n        with doc.create(Section(\"Details\", numbering=False)):\n            with doc.create(Subsection(\"Explanations:\", numbering=False)):\n                with doc.create(Itemize()) as itemize:\n                    for i in range(0, len(data.iloc[:, 0])):\n                        explanation_name = EXPLANATION_NAME_SHORT_TO_LONG[explanation_methods[i]]\n                        itemize.add_item(bold(explanation_name))\n                        doc.append(EXPLANATION_DESCRIPTION[str(explanation_name)][0])\n                        doc.append(\n                            hyperlink(\n                                EXPLANATION_DESCRIPTION[str(explanation_name)][1],\n                                EXPLANATION_DESCRIPTION[str(explanation_name)][2],\n                            )\n                        )\n                        doc.append(\"\\n\")\n                        doc.append(\"Explanation's parameters: \\n\")\n                        doc.append(NoEscape(r\"\\texttt{\"))\n                        doc.append(f\"{report_info['method_parameters'][explanation_methods[i]]} \\n\")\n                        doc.append(NoEscape(r\"}\"))\n            doc.append(NewPage())\n            with doc.create(Subsection(\"Metrics:\", numbering=False)):\n                with doc.create(Itemize()) as itemize:\n                    minus = 2\n                    for i in range(2, len(data.columns) - 1):\n                        if data.columns[i] == \"Time elapsed [s]\":\n                            minus += 1\n                        else:\n                            itemize.add_item(bold(METRIC_NAME_MEDIUM_TO_LONG[data.columns[i]]))\n                            doc.append(METRIC_DESCRIPTIONS[data.columns[i]][0])\n                            doc.append(\n                                hyperlink(\n                                    METRIC_DESCRIPTIONS[data.columns[i]][1], METRIC_DESCRIPTIONS[data.columns[i]][2]\n                                )\n                            )\n                            doc.append(\"\\n\")\n                            doc.append(\"Metric's parameters: \\n\")\n                            doc.append(NoEscape(r\"\\texttt{\"))\n                            doc.append(f\"{report_info['metric_parameters'][metrics_used[i-minus]]} \\n\")\n                            doc.append(NoEscape(r\"}\"))\n            with doc.create(Subsection(\"Aggregation parameters\", numbering=False)):\n                doc.append(NoEscape(r\"\\texttt{\"))\n                doc.append(report_info[\"aggregation_parameters\"])\n                doc.append(NoEscape(r\"}\"))\n        doc.append(NewPage())\n        with doc.create(Section(\"Examples of explanations\", numbering=False)):\n            with doc.create(Figure(position=\"!h\")) as mini_logo:\n                fig = report_info[\"fig_with_examples\"]\n                mini_logo.add_plot(fig=fig, width=f\"{21 - 2 * left_margin}cm\")\n\n        doc.generate_pdf(os.path.join(folder_path, \"report\"), clean_tex=False)\n\n    def get_best_explanation(self) -&gt; BestExplanation:\n\"\"\"\n        Returns an object with the selected best explanation method wrapped with a few additions, see BestExplanation for more details.\n        Returns (BestExplanation): BestExplanation object\n\n        \"\"\"\n        self._check_is_after_aggregation()\n        best_explanation_handler = self.explanation_handlers[self.best_explanation_name]\n        return BestExplanation(\n            attributions=best_explanation_handler.attributions,\n            explanation_function=best_explanation_handler.get_explanation_function(),\n            explanation_name=self.best_explanation_name,\n            explanation_function_parameters=best_explanation_handler.explanation_parameters,\n            metric_handlers=self.metric_handlers,\n            aggregation_parameters=self.aggregation_parameters,\n        )\n\n    def _get_info_for_report(self, labels: Union[Dict[int, str], None] = None) -&gt; Dict:\n        pp = pprint.PrettyPrinter(indent=4)\n        dict_list_for_df = []\n        methods = []\n        for k, v in self.first_aggregation_results.items():  # noqa: B007\n            methods.append(k)\n            dict_list_for_df.append(v)\n\n        metrics = list(self.first_aggregation_results[methods[0]].keys())\n\n        methods_full_names = [EXPLANATION_NAME_SHORT_TO_LONG[x] for x in methods]\n        metrics_full_names = [METRIC_NAME_LONG_TO_MEDIUM[x] for x in metrics]\n\n        df = pd.DataFrame(dict_list_for_df, index=methods_full_names)\n        df.columns = metrics_full_names\n        df[\"Time elapsed [s]\"] = pd.Series(\n            {EXPLANATION_NAME_SHORT_TO_LONG[k]: v for k, v in self.times_methods.items()}\n        )\n        agg_score = pd.Series(self.second_aggregation_results)\n        agg_score = agg_score.set_axis([EXPLANATION_NAME_SHORT_TO_LONG[x] for x in agg_score.index])\n\n        df[\"Agg. Score\"] = agg_score\n        df = df.sort_values([\"Agg. Score\", \"Time elapsed [s]\"], ascending=[False, True])\n\n        df[\"Rank\"] = np.arange(len(df)) + 1\n        cols = df.columns.tolist()\n        df = df[[cols[-1]] + cols[:-1]]\n        df = df.round(3)  # type: ignore\n        df.reset_index(inplace=True)\n        df.rename(columns={\"index\": \"Explanation Name\"}, inplace=True)\n\n        method_parameters = {k: pp.pformat(v.explanation_parameters) for k, v in self.explanation_handlers.items()}\n        metric_parameters = {k: pp.pformat(v.metric_parameters) for k, v in self.metric_handlers.items()}\n\n        fig = self._generate_plot_for_report(labels=labels)\n\n        metric_parameters = extract_function_names(metric_parameters)\n        method_parameters = extract_function_names(method_parameters)\n\n        aggregation_parameters = {\n            \"first_stage_aggregation_function\": self._first_stage_aggregation_function_name,\n            \"second_stage_aggregation_function\": self._second_stage_aggregation_function_name,\n            \"second_stage_aggregation_function_aggregation_parameters\": self.aggregation_parameters[\n                \"second_stage_aggregation_function_aggregation_parameters\"\n            ],\n        }\n        n_images = len(self.targets)\n        aggregation_parameters_str = pp.pformat(aggregation_parameters)\n        model_performance = self._evaluate_model_performance()\n        return {\n            \"execution_time\": self.times_total,\n            \"selected_method\": EXPLANATION_NAME_SHORT_TO_LONG[self.best_explanation_name],\n            \"result_dataframe\": df,\n            \"methods\": methods,\n            \"metrics\": metrics,\n            \"aggregation_parameters\": aggregation_parameters_str,\n            \"method_parameters\": method_parameters,\n            \"metric_parameters\": metric_parameters,\n            \"autoexplainer_version\": _get_package_version(),\n            \"date\": date.today(),\n            \"fig_with_examples\": fig,\n            \"n_images\": n_images,\n            **model_performance,\n        }\n\n    def _evaluate_model_performance(self) -&gt; Dict:\n        predictions = self.model(self.data).detach().cpu()\n        predicted_labels = predictions.argmax(axis=1).numpy()\n        y_true = self.targets.detach().cpu().numpy()\n        return {\n            \"model_acc\": round(accuracy_score(y_true, predicted_labels), 3),\n            \"model_f1_macro\": round(f1_score(y_true, predicted_labels, average=\"macro\"), 3),\n            \"model_bac\": round(balanced_accuracy_score(y_true, predicted_labels), 3),\n        }\n\n    def _generate_plot_for_report(\n        self, count_of_images: int = 10, labels: Union[Dict[int, str], None] = None\n    ) -&gt; plt.Figure:\n        number_of_explanations = len(self.explanation_handlers)\n        number_of_columns = number_of_explanations + 1\n        number_of_images = min(count_of_images, len(self.data))\n\n        if labels is None:\n            labels = {}\n\n        ids_of_images_to_show = []\n        classes = list(self.targets.unique().cpu().detach().numpy())\n        number_of_classes = len(classes)\n        images_per_class = int(number_of_images / number_of_classes)\n        for class_id in classes:\n            ids_of_images_from_this_class = [\n                i for i, x in enumerate(self.targets.cpu().detach().tolist()) if x == class_id\n            ]\n            ids_of_images_to_show += ids_of_images_from_this_class[:images_per_class]\n\n        number_of_images = len(ids_of_images_to_show)\n\n        fig = plt.figure(figsize=(10, 1.6 * number_of_images + 1))\n        grid = ImageGrid(\n            fig,\n            111,\n            nrows_ncols=(number_of_images, number_of_columns),\n            axes_pad=0,\n            share_all=True,\n        )\n        grid[0].set_xticks([])\n        grid[0].set_yticks([])\n\n        cmap = LinearSegmentedColormap.from_list(\"red-white-green\", [\"red\", \"white\", \"green\"])\n        vmin, vmax = -1, 1\n\n        images_to_plot: Dict[str, list] = {\"Original image\": []}\n\n        for original_image in self.data[ids_of_images_to_show]:\n            images_to_plot[\"Original image\"].append(normalize_image(torch_image_to_numpy_image(original_image)))\n\n        for explanation_name in self.explanation_handlers:\n            attributions_to_plot = self.explanation_handlers[explanation_name].attributions[ids_of_images_to_show]\n            full_explanation_name = EXPLANATION_NAME_SHORT_TO_LONG[explanation_name]\n            images_to_plot[full_explanation_name] = []\n            for attribution in attributions_to_plot:\n                attribution = torch_image_to_numpy_image(attribution)\n                if explanation_name == \"integrated_gradients\":\n                    attribution[attribution &gt; np.percentile(attribution, 99.5)] = np.percentile(attribution, 99.5)\n                    attribution[attribution &lt; np.percentile(attribution, 0.5)] = np.percentile(attribution, 0.5)\n                attribution_scaled = attribution / np.max(np.abs(attribution))\n                images_to_plot[full_explanation_name].append(attribution_scaled)\n\n        order_of_columns = [\"Original image\"] + [\n            EXPLANATION_NAME_SHORT_TO_LONG[explanation_name] for explanation_name in self.explanation_handlers\n        ]\n\n        for column_num, column_name in enumerate(order_of_columns):\n            grid[column_num].set_title(f\"{column_name}\", fontsize=11)\n            for row_num, image in enumerate(images_to_plot[column_name]):\n                if column_name == \"Original image\":\n                    grid[row_num * number_of_columns + column_num].imshow(image)\n                else:\n                    grid[row_num * number_of_columns + column_num].imshow(image, cmap=cmap, vmin=vmin, vmax=vmax)\n        for row_num in range(number_of_images):\n            image_for_prediction = self.data[ids_of_images_to_show[row_num]]\n            model_predition = self.model(image_for_prediction[None, :])\n            predicted_class = model_predition.max(1)[1].cpu().detach().numpy()[0]\n            predicted_class_softmax = (\n                torch.max(torch.nn.functional.softmax(model_predition, dim=1)).cpu().detach().numpy()\n            )\n\n            class_id = int(self.targets[ids_of_images_to_show[row_num]].cpu().detach().numpy())\n            grid[row_num * number_of_columns].set_ylabel(\n                r\"$\\bf{\"\n                + \"Real~class\"\n                + \"}$\"\n                + f\"\\n{labels.get(class_id, class_id)}\\n\"\n                + r\"$\\bf{\"\n                + \"Predicted~class\"\n                + \"}$\"\n                + f\"\\n{labels.get(predicted_class, predicted_class)}\\n\"\n                + r\"$\\bf{\"\n                + \"Predicted~score\"\n                + \"}$\"\n                + f\"\\n{predicted_class_softmax:.2f}\",\n                rotation=0,\n                size=\"large\",\n            )\n\n            grid[row_num * number_of_columns].yaxis.set_label_coords(-0.5, 0.2)\n\n        fig.suptitle(\"Examples of computed attributions\", fontsize=15, y=0.99)\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", category=UserWarning)\n            if number_of_images &gt; 4:\n                fig.tight_layout(rect=[0.05, 0, 1, 1])\n            else:\n                fig.tight_layout(rect=[0, 0, 1, 1])\n\n        return fig\n\n    def _check_is_after_evaluation(self) -&gt; None:\n        if self.raw_results is None or self.raw_results == {}:\n            raise ValueError(\"Methods are not evaluated yet. Please run .evaluate() first.\")  # noqa: TC003\n\n    def _check_is_after_aggregation(self) -&gt; None:\n        self._check_is_after_evaluation()\n        if (\n            self.best_explanation_name is None\n            or self.first_aggregation_results is None\n            or self.second_aggregation_results is None\n        ):\n            raise ValueError(\"Results are not aggregated yet. Please run .aggregate() first.\")  # noqa: TC003\n\n    def _check_aggregation_parameters(\n        self,\n        first_stage_aggregation_function_name: str,\n        second_stage_aggregation_function_name: str,\n        second_stage_aggregation_function_aggregation_parameters: Union[Dict[str, Any], None],\n    ) -&gt; None:\n        if not isinstance(first_stage_aggregation_function_name, str):\n            raise TypeError(\n                f\"First stage aggregation function name must be a string. Got {type(first_stage_aggregation_function_name)} instead.\"\n            )\n        if not isinstance(second_stage_aggregation_function_name, str):\n            raise TypeError(\n                f\"Second stage aggregation function name must be a string. Got {type(second_stage_aggregation_function_name)} instead.\"\n            )\n        if first_stage_aggregation_function_name not in self.KNOWN_FIRST_STAGE_AGGREGATION_FUNCTIONS:\n            raise ValueError(\n                f\"Unknown first stage aggregation function: {first_stage_aggregation_function_name}. Available functions: {list(self.KNOWN_FIRST_STAGE_AGGREGATION_FUNCTIONS.keys())}\"\n            )\n        if second_stage_aggregation_function_name not in self.KNOWN_SECOND_STAGE_AGGREGATION_FUNCTIONS:\n            raise ValueError(\n                f\"Unknown second stage aggregation function: {second_stage_aggregation_function_name}. Available functions: {list(self.KNOWN_SECOND_STAGE_AGGREGATION_FUNCTIONS.keys())}\"\n            )\n        if second_stage_aggregation_function_aggregation_parameters is not None:\n            if not isinstance(second_stage_aggregation_function_aggregation_parameters, dict):\n                raise TypeError(\n                    f\"Second stage aggregation function parameters must be provided as a dictionary. Got {type(second_stage_aggregation_function_aggregation_parameters)} instead.\"\n                )\n\n    def _check_model(self, model: torch.nn.Module) -&gt; None:\n        if not isinstance(model, torch.nn.Module):\n            raise TypeError(\"Model must be of type torch.nn.Module.\")  # noqa: TC003\n\n    def _check_data(self, data: torch.Tensor, targets: torch.Tensor) -&gt; None:\n        if not isinstance(data, torch.Tensor):\n            raise TypeError(\"Data must be of type torch.Tensor.\")  # noqa: TC003\n        if len(data.shape) != 4:\n            raise ValueError(\"Data must be of shape (N, C, H, W).\")  # noqa: TC003\n        if not isinstance(targets, torch.Tensor):\n            raise TypeError(\"Targets must be of type torch.Tensor.\")  # noqa: TC003\n        if len(targets.shape) != 1:\n            raise ValueError(\"Targets must be of shape (N,).\")  # noqa: TC003\n        if data.shape[0] != targets.shape[0]:\n            raise ValueError(\"Data and targets must have the same number of observations.\")  # noqa: TC003\n        if torch.any(torch.isnan(data)):\n            raise ValueError(\"Provided data has NaN values.\")\n        if torch.any(torch.isnan(targets)):\n            raise ValueError(\"Targets have NaN values.\")\n\n    def _check_method_and_metric_names(\n        self, method_names: Union[List[str], None], metric_names: Union[List[str], None]\n    ) -&gt; None:\n        if method_names is not None:\n            for method_name in method_names:\n                if not isinstance(method_name, str):\n                    raise ValueError(\"Method names must be strings.\")  # noqa: TC003\n                if method_name not in self.KNOWN_EXPLANATION_HANDLERS:\n                    raise ValueError(\n                        f\"Unknown explanation method: {method_name}. Available explanation methods: {list(self.KNOWN_EXPLANATION_HANDLERS.keys())}\"\n                    )  # noqa: TC003\n        if metric_names is not None:\n            for metric_name in metric_names:\n                if not isinstance(metric_name, str):\n                    raise ValueError(\"Metric names must be strings.\")  # noqa: TC003\n                if metric_name not in self.KNOWN_METRIC_HANDLERS:\n                    raise ValueError(\n                        f\"Unknown metric: {metric_name}. Available metrics: {list(self.KNOWN_METRIC_HANDLERS.keys())}\"\n                    )  # noqa: TC003\n\n    def _check_method_and_metric_params_dicts(\n        self,\n        method_names: List[str],\n        metric_names: List[str],\n        method_params: Union[Dict, None],\n        metric_params: Union[Dict, None],\n    ) -&gt; None:\n        if method_params is not None:\n            if not isinstance(method_params, dict):\n                raise TypeError(\"Explanation parameters must be a dictionary.\")  # noqa: TC003\n            for method_name, method_param in method_params.items():\n                if method_name not in self.KNOWN_EXPLANATION_HANDLERS:\n                    raise ValueError(\n                        f\"Unknown explanation method: {method_name}. Available explanation methods: {list(self.KNOWN_EXPLANATION_HANDLERS.keys())}\"\n                    )\n                if method_name not in method_names:\n                    warnings.warn(\n                        f\"Explanation method {method_name} is not in the list of methods to evaluate but the parameters were set for this method.\",\n                        UserWarning,\n                    )\n                if not isinstance(method_param, dict):\n                    raise TypeError(\n                        f\"Explanation method parameters must be provided as a dictionary. Got {type(method_param)} instead.\"\n                    )\n        if metric_params is not None:\n            if not isinstance(metric_params, dict):\n                raise TypeError(\"Metric parameters must be a dictionary.\")  # noqa: TC003\n            for metric_name, metric_param in metric_params.items():\n                if metric_name not in self.KNOWN_METRIC_HANDLERS:\n                    raise ValueError(\n                        f\"Unknown metric: {metric_name}. Available metrics: {list(self.KNOWN_METRIC_HANDLERS.keys())}\"\n                    )\n                if metric_name not in metric_names:\n                    warnings.warn(\n                        f\"Metric {metric_name} is not in the list of metrics to evaluate but the parameters were set for this metric.\",\n                        UserWarning,\n                    )\n                if not isinstance(metric_param, dict):\n                    raise TypeError(\n                        f\"Metric parameters must be provided as a dictionary. Got {type(metric_param)} instead.\"\n                    )\n\n    def _set_seed(self, seed: int) -&gt; None:\n\"\"\"\n        Sets seed for all random number generators.\n        Args:\n            seed (int): Seed for random number generators.\n        \"\"\"\n        np.random.seed(seed)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n</code></pre>"},{"location":"api_docs/#autoexplainer.autoexplainer.AutoExplainer.__init__","title":"<code>__init__(model: torch.nn.Module, data: torch.Tensor, targets: torch.Tensor, device: str = 'cpu', seed: int = 42)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>torch.nn.Module</code> <p>Convolutional neural network to be explained. On this model some explanation and metric parameters will be inferred.</p> required <code>data</code> <code>torch.Tensor</code> <p>Data that will be used for explanation method evaluation. shape: (N, C, H, W).</p> required <code>targets</code> <code>torch.Tensor</code> <p>Labels for provided data. Encoded as integer vector with shape (N,).</p> required Source code in <code>autoexplainer/autoexplainer.py</code> <pre><code>def __init__(\n    self, model: torch.nn.Module, data: torch.Tensor, targets: torch.Tensor, device: str = \"cpu\", seed: int = 42\n):\n\"\"\"\n\n    Args:\n        model (torch.nn.Module): Convolutional neural network to be explained. On this model some explanation and metric\n            parameters will be inferred.\n        data (torch.Tensor): Data that will be used for explanation method evaluation. shape: (N, C, H, W).\n        targets (torch.Tensor): Labels for provided data. Encoded as integer vector with shape (N,).\n    \"\"\"\n    self._check_model(model)\n    self._check_data(data, targets)\n    self._second_stage_aggregation_function_name = None\n    self._first_stage_aggregation_function_name = None\n\n    self.explanation_handlers: Dict = None\n    self.metric_handlers: Dict = None\n    self.first_aggregation_results: Dict = None\n    self.second_aggregation_results: Dict = None\n    self.best_explanation_name: str = None\n    self.aggregation_parameters: Dict = None\n\n    model = model.to(device)\n    model.eval()\n    self.model = fix_relus_in_model(model)\n    self.data = data.to(device)\n    self.targets = targets.to(device)\n    self.raw_results: Dict = {}\n    self.times_methods: Dict = {}\n    self.times_metrics: Dict = {}\n    self.times_metrics_aggregated: Dict = {}\n    self.times_total = 0.0\n    self.device = device\n    self._set_seed(seed)\n</code></pre>"},{"location":"api_docs/#autoexplainer.autoexplainer.AutoExplainer.aggregate","title":"<code>aggregate(first_stage_aggregation_function_name: str = 'mean', second_stage_aggregation_function_name: str = 'rank_based', second_stage_aggregation_function_aggregation_parameters: Dict = None) -&gt; None</code>","text":"<p>Aggregates raw result computed in .evaluate() method in two steps. First, aggregates metric scores across provided observations, i.e. each explanation method has a  value for each metric. Secondly, aggregates scores across available metrics, i.e. each explanation method has a single value that represents overall quality.</p> <p>Stores both aggregation steps in the attributes <code>first_aggregation_results</code> and <code>second_aggregation_results</code>.</p> <p>Parameters:</p> Name Type Description Default <code>first_stage_aggregation_function_name</code> <code>{\"mean\", \"median\", \"min\",\"max\"}</code> <p>Name of the function for the first stage aggregation.</p> <code>'mean'</code> <code>second_stage_aggregation_function_name</code> <code>{\"mean\", \"median\", \"min\",\"max\"}</code> <p>Name of the function for second stage aggregaton.</p> <code>'rank_based'</code> <code>second_stage_aggregation_function_aggregation_parameters</code> <code>Dict</code> <p>Parameters for the second stage aggregation function.</p> <code>None</code> Source code in <code>autoexplainer/autoexplainer.py</code> <pre><code>def aggregate(\n    self,\n    first_stage_aggregation_function_name: str = \"mean\",\n    second_stage_aggregation_function_name: str = \"rank_based\",\n    second_stage_aggregation_function_aggregation_parameters: Dict = None,\n) -&gt; None:\n\"\"\"\n    Aggregates raw result computed in .evaluate() method in two steps. First, aggregates metric scores across\n    provided observations, i.e. each explanation method has a  value for each metric. Secondly, aggregates\n    scores across available metrics, i.e. each explanation method has a single value that represents overall quality.\n\n    Stores both aggregation steps in the attributes ``first_aggregation_results`` and ``second_aggregation_results``.\n\n    Args:\n        first_stage_aggregation_function_name ({\"mean\", \"median\", \"min\",\"max\"}): Name of the function for the first stage aggregation.\n        second_stage_aggregation_function_name ({\"mean\", \"median\", \"min\",\"max\"}): Name of the function for second stage aggregaton.\n        second_stage_aggregation_function_aggregation_parameters (Dict): Parameters for the second stage aggregation function.\n\n    \"\"\"\n\n    self._check_is_after_evaluation()\n    self._check_aggregation_parameters(\n        first_stage_aggregation_function_name,\n        second_stage_aggregation_function_name,\n        second_stage_aggregation_function_aggregation_parameters,\n    )\n    self._first_stage_aggregation_function_name = first_stage_aggregation_function_name\n    self._second_stage_aggregation_function_name = second_stage_aggregation_function_name\n\n    if second_stage_aggregation_function_aggregation_parameters is None:\n        second_stage_aggregation_function_aggregation_parameters = {}\n    self.first_aggregation_results = self.KNOWN_FIRST_STAGE_AGGREGATION_FUNCTIONS[\n        first_stage_aggregation_function_name\n    ](self.raw_results)\n    self.second_aggregation_results = self.KNOWN_SECOND_STAGE_AGGREGATION_FUNCTIONS[\n        second_stage_aggregation_function_name\n    ](self.first_aggregation_results, second_stage_aggregation_function_aggregation_parameters)\n    sorted_results = sorted(self.second_aggregation_results.items(), key=lambda x: x[1], reverse=True)\n    if len(sorted_results) &gt; 1:\n        best_result, second_best_result = sorted_results[0], sorted_results[1]\n        if best_result[1] == second_best_result[1]:\n            if self.times_methods[best_result[0]] &gt; self.times_methods[second_best_result[0]]:\n                best_result = second_best_result\n        self.best_explanation_name = best_result[0]\n    else:\n        self.best_explanation_name = sorted_results[0][0]\n\n    self.aggregation_parameters = {\n        \"first_stage_aggregation_function\": self.KNOWN_FIRST_STAGE_AGGREGATION_FUNCTIONS[\n            first_stage_aggregation_function_name\n        ],\n        \"second_stage_aggregation_function\": self.KNOWN_SECOND_STAGE_AGGREGATION_FUNCTIONS[\n            second_stage_aggregation_function_name\n        ],\n        \"second_stage_aggregation_function_aggregation_parameters\": second_stage_aggregation_function_aggregation_parameters,\n    }\n</code></pre>"},{"location":"api_docs/#autoexplainer.autoexplainer.AutoExplainer.evaluate","title":"<code>evaluate(explanations: List[str] = None, metrics: List[str] = None, explanation_params: Dict = None, metrics_params: Dict = None) -&gt; None</code>","text":"<p>Evaluates explanation methods. Stores results in <code>.raw_results</code> attribute.</p> <p>Parameters:</p> Name Type Description Default <code>explanations</code> <code>List[str]</code> <p>List of names of explanation methods to be evaluated.                       By default, uses all available explanation methods.                       Accepts lists with subset of: <code>{\"saliency\", \"grad_cam\", \"integrated_gradients\", \"kernel_shap\"}</code>.</p> <code>None</code> <code>metrics</code> <code>List[str]</code> <p>List of names of evaluation metrics to be used. By default, uses all available metrics.                 Accepts lists with subset of: <code>{\"irof\", \"sparseness\", \"average_sensitivity\", \"faithfulness_estimate\"}</code>.</p> <code>None</code> <code>explanation_params</code> <code>Dict[str, Dict]</code> <p>Allows to override default parameters of selected explanation functions.                                 Accept Dictionary with form <code>{\"explanation_name\": &lt;Dictionary with parameters&gt;}</code>.                                 See corresponding ExplanationHandler to see what parameters are accepted.</p> <code>None</code> <code>metrics_params</code> <code>Dict[str, Dict]</code> <p>Allows to override default parameters of selected metrics.                               Accept Dictionary with form <code>{\"metric_name\": &lt;Dictionary with parameters&gt;}</code>.                               See corresponding MetricHandler to see what parameters are accepted.</p> <code>None</code> Source code in <code>autoexplainer/autoexplainer.py</code> <pre><code>def evaluate(\n    self,\n    explanations: List[str] = None,\n    metrics: List[str] = None,\n    explanation_params: Dict = None,\n    metrics_params: Dict = None,\n) -&gt; None:\n\"\"\"\n    Evaluates explanation methods. Stores results in ``.raw_results`` attribute.\n    Args:\n        explanations (List[str]): List of names of explanation methods to be evaluated.\n                                  By default, uses all available explanation methods.\n                                  Accepts lists with subset of: ``{\"saliency\", \"grad_cam\", \"integrated_gradients\", \"kernel_shap\"}``.\n        metrics (List[str]): List of names of evaluation metrics to be used. By default, uses all available metrics.\n                            Accepts lists with subset of: ``{\"irof\", \"sparseness\", \"average_sensitivity\", \"faithfulness_estimate\"}``.\n        explanation_params (Dict[str, Dict]): Allows to override default parameters of selected explanation functions.\n                                            Accept Dictionary with form ``{\"explanation_name\": &lt;Dictionary with parameters&gt;}``.\n                                            See corresponding ExplanationHandler to see what parameters are accepted.\n        metrics_params (Dict[str, Dict]): Allows to override default parameters of selected metrics.\n                                          Accept Dictionary with form ``{\"metric_name\": &lt;Dictionary with parameters&gt;}``.\n                                          See corresponding MetricHandler to see what parameters are accepted.\n\n    \"\"\"\n    self._check_method_and_metric_names(explanations, metrics)\n    if explanations is None:\n        explanations = self.KNOWN_EXPLANATION_HANDLERS\n    if metrics is None:\n        metrics = self.KNOWN_METRIC_HANDLERS\n    self._check_method_and_metric_params_dicts(explanations, metrics, explanation_params, metrics_params)\n    if explanation_params is None:\n        explanation_params = {}\n    if metrics_params is None:\n        metrics_params = {}\n\n    print(\"\\nPreparing explanation methods and metric handlers...\\n\")\n\n    self.explanation_handlers = {\n        explanation_name: self.KNOWN_EXPLANATION_HANDLERS[explanation_name](\n            self.model, self.data, self.targets, explanation_params.get(explanation_name)\n        )\n        for explanation_name in explanations\n    }\n    self.metric_handlers = {\n        metric_name: self.KNOWN_METRIC_HANDLERS[metric_name](\n            self.model, self.data, self.targets, metrics_params.get(metric_name)\n        )\n        for metric_name in metrics\n    }\n    self.times_metrics = {metric_name: {} for metric_name in metrics}\n\n    print(\"\\tNumber of explanation methods to evaluate: \", len(self.explanation_handlers))\n    print(\n        \"\\tExplanation methods selected: \"\n        + f\"{', '.join([EXPLANATION_NAME_SHORT_TO_LONG[x] for x in list(self.explanation_handlers.keys())])}\"\n    )\n    print(\"\")\n    print(\"\\tNumber of metrics used during evaluation: \", len(self.metric_handlers))\n    print(\n        \"\\tMetrics selected: \"\n        + f\"{', '.join([METRIC_NAME_SHORT_TO_LONG[x] for x in list(self.metric_handlers.keys())])}\"\n    )\n\n    pbar = tqdm.tqdm(self.explanation_handlers.items(), desc=\"Creating attributions\")\n    for explanation_name, explanation_handler in pbar:\n        start_time = time.time()\n        pbar.set_description(f\"Creating attributions for {explanation_name}\")\n        explanation_handler.explain(model=self.model, data=self.data, targets=self.targets)\n        self.times_methods[explanation_name] = round(time.time() - start_time, 3)\n\n    for explanation_name in self.explanation_handlers.keys():\n        self.raw_results[explanation_name] = {}\n\n    print(\"Creating attribution finished. Starting evaluation.\")\n    print(\"Evaluation may take a very long time, please be patient...\")\n\n    pbar = tqdm.tqdm(\n        itertools.product(self.metric_handlers.items(), self.explanation_handlers.items()),\n        total=len(self.metric_handlers) * len(self.explanation_handlers),\n        desc=\"Evaluating metrics\",\n    )\n    for (metric_name, metric_handler), (explanation_name, explanation_handler) in pbar:\n        start_time = time.time()\n        pbar.set_description(f\"Evaluating: method {explanation_name} and metric {metric_name}\")\n        self.raw_results[explanation_name][metric_name] = metric_handler.compute_metric_values(\n            model=self.model,\n            data=self.data,\n            targets=self.targets,\n            attributions=explanation_handler.attributions.to(next(self.model.parameters()).device),\n            explanation_func=explanation_handler.get_explanation_function(),\n        )\n        self.times_metrics[metric_name][explanation_name] = round(\n            time.time() - start_time + self.times_methods[explanation_name], 3\n        )\n\n    self.times_metrics_aggregated = {\n        metric_name: round(sum(self.times_metrics[metric_name].values()), 3)\n        for metric_name in self.times_metrics.keys()\n    }\n    self.times_total = round(sum(self.times_metrics_aggregated.values()), 3)\n\n    print(f\"Evaluating metrics finished after {self.times_total} seconds.\")\n</code></pre>"},{"location":"api_docs/#autoexplainer.autoexplainer.AutoExplainer.get_best_explanation","title":"<code>get_best_explanation() -&gt; BestExplanation</code>","text":"<p>Returns an object with the selected best explanation method wrapped with a few additions, see BestExplanation for more details. Returns (BestExplanation): BestExplanation object</p> Source code in <code>autoexplainer/autoexplainer.py</code> <pre><code>def get_best_explanation(self) -&gt; BestExplanation:\n\"\"\"\n    Returns an object with the selected best explanation method wrapped with a few additions, see BestExplanation for more details.\n    Returns (BestExplanation): BestExplanation object\n\n    \"\"\"\n    self._check_is_after_aggregation()\n    best_explanation_handler = self.explanation_handlers[self.best_explanation_name]\n    return BestExplanation(\n        attributions=best_explanation_handler.attributions,\n        explanation_function=best_explanation_handler.get_explanation_function(),\n        explanation_name=self.best_explanation_name,\n        explanation_function_parameters=best_explanation_handler.explanation_parameters,\n        metric_handlers=self.metric_handlers,\n        aggregation_parameters=self.aggregation_parameters,\n    )\n</code></pre>"},{"location":"api_docs/#autoexplainer.autoexplainer.AutoExplainer.to_html","title":"<code>to_html(file_path: str, model_name: str = None, dataset_name: str = None, labels: Dict[int, str] = None) -&gt; None</code>","text":"<p>Generates evaluation report as HTML file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Target file path.</p> required <code>model_name</code> <code>str</code> <p>Name of model to show inside report.</p> <code>None</code> <code>dataset_name</code> <code>str</code> <p>Name of dataset to show inside report.</p> <code>None</code> <code>labels</code> <code>Dict[int, str]</code> <p>Mapping between class number and class names. e.g. <code>{0:\"dog\", 1:\"cat\", 2:\"fish\"}</code> for labels                     inside report.</p> <code>None</code> Source code in <code>autoexplainer/autoexplainer.py</code> <pre><code>def to_html(\n    self, file_path: str, model_name: str = None, dataset_name: str = None, labels: Dict[int, str] = None\n) -&gt; None:\n\"\"\"\n    Generates evaluation report as HTML file.\n    Args:\n        file_path (str): Target file path.\n        model_name (str): Name of model to show inside report.\n        dataset_name (str): Name of dataset to show inside report.\n        labels (Dict[int,str]): Mapping between class number and class names. e.g. ``{0:\"dog\", 1:\"cat\", 2:\"fish\"}`` for labels\n                                inside report.\n    \"\"\"\n    assert self.first_aggregation_results is not None, \"Aggregated results are needed for report generation.\"\n    assert self.second_aggregation_results is not None, \"Aggregated results are needed for report generation.\"\n\n    environment = Environment(loader=PackageLoader(\"autoexplainer\"))\n    template = environment.get_template(\"report.html\")\n\n    report_info = self._get_info_for_report(labels=labels)\n\n    pic_io_bytes = io.BytesIO()\n    fig = report_info[\"fig_with_examples\"]\n    fig.savefig(pic_io_bytes, format=\"png\")\n    pic_io_bytes.seek(0)\n    pic_hash = base64.b64encode(pic_io_bytes.read())\n\n    _, _, *float_columns, _ = report_info[\"result_dataframe\"].columns\n\n    html_table = (\n        report_info[\"result_dataframe\"]\n        .style.set_properties(\n            subset=[\"Agg. Score\", \"Explanation Name\"], **{\"font-weight\": \"bold\", \"text-align\": \"center\"}\n        )\n        .set_properties(border=0)\n        .hide_index()\n        .format(\"{:.3f}\", subset=float_columns)\n        .render()\n    )\n\n    rendered = template.render(\n        model_name=model_name,\n        dataset_name=dataset_name,\n        dataframe_html=html_table,\n        pic_hash=pic_hash.decode(),\n        **report_info,\n    )\n    with open(file_path, mode=\"w\", encoding=\"utf-8\") as results:\n        results.write(rendered)\n</code></pre>"},{"location":"api_docs/#autoexplainer.autoexplainer.AutoExplainer.to_pdf","title":"<code>to_pdf(folder_path: str = '', model_name: str = 'name of the model', dataset_name: str = 'name of the dataset', labels: Dict[int, str] = None) -&gt; None</code>","text":"<p>Creates PDF report from dict stored in the attribute <code>first_aggregation_results</code>. Needs Latex packages installed to run - see README.</p> <p>Parameters:</p> Name Type Description Default <code>folder_path</code> <code>str</code> <p>Path to directory, where the reports (PDF and tex) should be created.</p> <code>''</code> <code>model_name</code> <code>str</code> <p>Name of model to show inside report.</p> <code>'name of the model'</code> <code>dataset_name</code> <code>str</code> <p>Name of dataset to show inside report.</p> <code>'name of the dataset'</code> <code>labels</code> <code>Dict[int, str]</code> <p>Mapping between class number and class names. e.g. <code>{0:\"dog\", 1:\"cat\", 2:\"fish\"}</code> for labels                     inside report.</p> <code>None</code> Source code in <code>autoexplainer/autoexplainer.py</code> <pre><code>def to_pdf(\n    self,\n    folder_path: str = \"\",\n    model_name: str = \"name of the model\",\n    dataset_name: str = \"name of the dataset\",\n    labels: Dict[int, str] = None,\n) -&gt; None:\n\n\"\"\"\n    Creates PDF report from dict stored in the attribute ``first_aggregation_results``.\n    Needs Latex packages installed to run - see README.\n\n    Args:\n        folder_path (str): Path to directory, where the reports (PDF and tex) should be created.\n        model_name (str): Name of model to show inside report.\n        dataset_name (str): Name of dataset to show inside report.\n        labels (Dict[int,str]): Mapping between class number and class names. e.g. ``{0:\"dog\", 1:\"cat\", 2:\"fish\"}`` for labels\n                                inside report.\n\n    \"\"\"\n    self._check_is_after_aggregation()\n\n    tex_file = os.path.join(folder_path, \"report.tex\")\n    pdf_file = os.path.join(folder_path, \"report.pdf\")\n\n    if os.path.exists(tex_file):\n        os.remove(tex_file)\n    if os.path.exists(pdf_file):\n        os.remove(pdf_file)\n\n    report_info = self._get_info_for_report(labels=labels)\n\n    left_margin = 2\n    max_nr_columns_in_table = 5\n    geometry_options = {\"tmargin\": \"2cm\", \"lmargin\": f\"{left_margin}cm\"}\n    doc = Document(geometry_options=geometry_options)\n    doc.preamble.append(Command(\"title\", \"AutoeXplainer Report\"))\n    doc.preamble.append(Command(\"date\", \"\"))\n    doc.packages.append(Package(\"hyperref\"))\n    doc.packages.append(Package(\"booktabs\"))\n    doc.append(NoEscape(r\"\\maketitle\"))\n\n    results = report_info[\"result_dataframe\"]\n\n    metric_name_copy = copy.deepcopy(METRIC_NAME_SHORT_TO_LONG)\n    metric_name_copy[\"explanation_name\"] = \"explanation name\"\n    metric_name_copy[\"Rank\"] = \"Rank\"\n    metric_name_copy[\"Agg. Score\"] = \"Agg. Score\"\n    explanation_methods = report_info[\"methods\"]\n    metrics = report_info[\"metrics\"]\n    metrics_used = copy.deepcopy(metrics)\n\n    metrics = [\"explanation name\", \"Rank\"] + metrics + [\"Agg. Score\"]\n    data = copy.deepcopy(results)\n\n    def hyperlink(url: str, text: str) -&gt; NoEscape:  # type: ignore\n        return NoEscape(r\"\\href{\" + url + \"}{\" + escape_latex(text) + \"}\")\n\n    # create content of  the Document\n    with doc.create(Section(\"General information\", numbering=False)):\n        doc.append(bold(\"Model name: \"))\n        doc.append(italic(f\"{model_name} \\n\"))\n        doc.append(bold(\"Dataset name: \"))\n        doc.append(italic(f\"{dataset_name} \\n\"))\n        doc.append(bold(\"Execution time: \"))\n        doc.append(italic(f\"{report_info['execution_time']} s \\n\"))\n        doc.append(bold(\"Package version: \"))\n        doc.append(italic(f\"{report_info['autoexplainer_version']} \\n\"))\n        doc.append(bold(\"Date: \"))\n        doc.append(italic(f\"{report_info['date']} \\n\"))\n        doc.append(bold(\"Selected method: \"))\n        doc.append(italic(f\"{report_info['selected_method']} \\n\"))\n        doc.append(bold(\"Number of images: \"))\n        doc.append(italic(f\"{report_info['n_images']}\"))\n\n    with doc.create(Section(\"Model performance\", numbering=False)):\n        doc.append(bold(\"Accuracy: \"))\n        doc.append(italic(f\"{report_info['model_acc']} \\n\"))\n        doc.append(bold(\"F1 macro: \"))\n        doc.append(italic(f\"{report_info['model_f1_macro']} \\n\"))\n        doc.append(bold(\"Balanced accuracy: \"))\n        doc.append(italic(f\"{report_info['model_bac']} \\n\"))\n\n    with doc.create(Section(\"Table of results\", numbering=False)):\n        doc.append(NoEscape(r\"\\begin{footnotesize}\"))\n        doc.append(NoEscape(r\"\\begin{flushleft} \"))\n        doc.append(NoEscape(report_info[\"result_dataframe\"].to_latex(index=False)))\n        doc.append(NoEscape(r\"\\end{flushleft}\"))\n        doc.append(NoEscape(r\"\\end{footnotesize}\"))\n        doc.append(bold(\"Table description \\n\"))\n        doc.append(\n            \"Arrow next to the metric names indicates whether larger or smaller values of metric are better. Time elapsed shows time that was required for computation of attribution for given batch of images. When there is a tie in Aggregated Score, the best metric is chosen based on computation time.\"\n        )\n\n    doc.append(NewPage())\n    with doc.create(Section(\"Details\", numbering=False)):\n        with doc.create(Subsection(\"Explanations:\", numbering=False)):\n            with doc.create(Itemize()) as itemize:\n                for i in range(0, len(data.iloc[:, 0])):\n                    explanation_name = EXPLANATION_NAME_SHORT_TO_LONG[explanation_methods[i]]\n                    itemize.add_item(bold(explanation_name))\n                    doc.append(EXPLANATION_DESCRIPTION[str(explanation_name)][0])\n                    doc.append(\n                        hyperlink(\n                            EXPLANATION_DESCRIPTION[str(explanation_name)][1],\n                            EXPLANATION_DESCRIPTION[str(explanation_name)][2],\n                        )\n                    )\n                    doc.append(\"\\n\")\n                    doc.append(\"Explanation's parameters: \\n\")\n                    doc.append(NoEscape(r\"\\texttt{\"))\n                    doc.append(f\"{report_info['method_parameters'][explanation_methods[i]]} \\n\")\n                    doc.append(NoEscape(r\"}\"))\n        doc.append(NewPage())\n        with doc.create(Subsection(\"Metrics:\", numbering=False)):\n            with doc.create(Itemize()) as itemize:\n                minus = 2\n                for i in range(2, len(data.columns) - 1):\n                    if data.columns[i] == \"Time elapsed [s]\":\n                        minus += 1\n                    else:\n                        itemize.add_item(bold(METRIC_NAME_MEDIUM_TO_LONG[data.columns[i]]))\n                        doc.append(METRIC_DESCRIPTIONS[data.columns[i]][0])\n                        doc.append(\n                            hyperlink(\n                                METRIC_DESCRIPTIONS[data.columns[i]][1], METRIC_DESCRIPTIONS[data.columns[i]][2]\n                            )\n                        )\n                        doc.append(\"\\n\")\n                        doc.append(\"Metric's parameters: \\n\")\n                        doc.append(NoEscape(r\"\\texttt{\"))\n                        doc.append(f\"{report_info['metric_parameters'][metrics_used[i-minus]]} \\n\")\n                        doc.append(NoEscape(r\"}\"))\n        with doc.create(Subsection(\"Aggregation parameters\", numbering=False)):\n            doc.append(NoEscape(r\"\\texttt{\"))\n            doc.append(report_info[\"aggregation_parameters\"])\n            doc.append(NoEscape(r\"}\"))\n    doc.append(NewPage())\n    with doc.create(Section(\"Examples of explanations\", numbering=False)):\n        with doc.create(Figure(position=\"!h\")) as mini_logo:\n            fig = report_info[\"fig_with_examples\"]\n            mini_logo.add_plot(fig=fig, width=f\"{21 - 2 * left_margin}cm\")\n\n    doc.generate_pdf(os.path.join(folder_path, \"report\"), clean_tex=False)\n</code></pre>"},{"location":"api_docs/#metric-handlers","title":"Metric handlers","text":""},{"location":"api_docs/#autoexplainer.metrics.AvgSensitivityHandler","title":"<code>AvgSensitivityHandler</code>","text":"<p>         Bases: <code>MetricHandler</code></p> <p>Metric Handler for Average Sensivity metric (Yeh et al., 2019).</p> <p>Measures the average sensitivity of an explanation using a Monte Carlo sampling-based approximation.</p> <p>Dictionary with parameters to override must be in the form:</p> <pre><code>metric_parameters = {\"init\": &lt;dictionary with parameters used in metric's __init__&gt;,\n                   \"call\": &lt;dictionary with parameters used in metric's __call__&gt;}\n</code></pre> <p>Parameters accepted in <code>metric_parameters</code>:</p> <p>\"init\":</p> <ul> <li><code>abs</code>: a bool stating if absolute operation should be taken on the attributions</li> <li><code>normalise</code>: a bool stating if the attributions should be normalised</li> <li><code>normalise_func</code>: a Callable that make a normalising transformation of the attributions</li> <li><code>lower_bound</code> (float): lower Bound of Perturbation, default=0.2</li> <li><code>upper_bound</code> (None, float): upper Bound of Perturbation, default=None</li> <li><code>nr_samples</code> (integer): the number of samples iterated, default=200.</li> <li><code>norm_numerator</code> (callable): function for norm calculations on the numerator, default=fro_norm.</li> <li><code>norm_denominator</code> (callable): function for norm calculations on the denominator, default=fro_norm.</li> <li><code>perturb_func</code> (callable): input perturbation function, default=uniform_noise.</li> <li><code>similarity_func</code> (callable): similarity function applied to compare input and perturbed input.</li> </ul> <p>\"call\": No parameters are used.</p> Source code in <code>autoexplainer/metrics.py</code> <pre><code>class AvgSensitivityHandler(MetricHandler):\n\"\"\"\n    Metric Handler for Average Sensivity metric [(Yeh et al., 2019)](https://arxiv.org/abs/1901.09392).\n\n    Measures the average sensitivity of an explanation using a Monte Carlo sampling-based approximation.\n\n    Dictionary with parameters to override must be in the form:\n    ```\n    metric_parameters = {\"init\": &lt;dictionary with parameters used in metric's __init__&gt;,\n                       \"call\": &lt;dictionary with parameters used in metric's __call__&gt;}\n    ```\n\n    Parameters accepted in `metric_parameters`:\n\n    **\"init\"**:\n\n    - `abs`: a bool stating if absolute operation should be taken on the attributions\n    - `normalise`: a bool stating if the attributions should be normalised\n    - `normalise_func`: a Callable that make a normalising transformation of the attributions\n    - `lower_bound` (float): lower Bound of Perturbation, default=0.2\n    - `upper_bound` (None, float): upper Bound of Perturbation, default=None\n    - `nr_samples` (integer): the number of samples iterated, default=200.\n    - `norm_numerator` (callable): function for norm calculations on the numerator, default=fro_norm.\n    - `norm_denominator` (callable): function for norm calculations on the denominator, default=fro_norm.\n    - `perturb_func` (callable): input perturbation function, default=uniform_noise.\n    - `similarity_func` (callable): similarity function applied to compare input and perturbed input.\n\n    **\"call\"**: No parameters are used.\n\n    \"\"\"\n\n    def __init__(\n        self, model: torch.nn.Module, data: torch.Tensor, targets: torch.Tensor, metric_parameters: Dict = None\n    ) -&gt; None:\n        self.metric_parameters = {\"init\": self._infer_metric_parameters(model, data, targets), \"call\": {}}\n        self.metric_parameters[\"init\"] = self._add_hide_output_parameters_to_dict(self.metric_parameters[\"init\"])\n        if str(next(model.parameters()).device) != \"cpu\":\n            self.metric_parameters[\"call\"] = {\"device\": \"cuda\"}\n        if metric_parameters is not None:\n            update_dictionary(self.metric_parameters, metric_parameters)\n        self.metric = quantus.AvgSensitivity(**self.metric_parameters[\"init\"])\n\n    def _infer_metric_parameters(self, model: torch.nn.Module, data: torch.Tensor, targets: torch.Tensor) -&gt; Dict:\n        parameters = {\n            \"normalise\": True,\n            \"nr_samples\": 20,\n            \"lower_bound\": 0.2,\n            \"norm_numerator\": quantus.fro_norm,\n            \"norm_denominator\": quantus.fro_norm,\n            \"perturb_func\": quantus.uniform_noise,\n            \"similarity_func\": quantus.difference,\n            \"perturb_radius\": 0.2,\n        }\n        return parameters\n</code></pre>"},{"location":"api_docs/#autoexplainer.metrics.FaithfulnessEstimateHandler","title":"<code>FaithfulnessEstimateHandler</code>","text":"<p>         Bases: <code>MetricHandler</code></p> <p>Metric handler for Faithfulness Estimate metric (Alvarez-Melis et al., 2018).</p> <p>Computes the correlation between probability drops and attribution scores on various points.</p> <p>Dictionary with parameters to override must be in the form:</p> <pre><code>metric_parameters = {\"init\": &lt;dictionary with parameters used in metric's __init__&gt;,\n                   \"call\": &lt;dictionary with parameters used in metric's __call__&gt;}\n</code></pre> <p>Parameters accepted in <code>metric_parameters</code>:</p> <p>\"init\":</p> <ul> <li><code>abs</code>: a bool stating if absolute operation should be taken on the attributions</li> <li><code>normalise</code>: a bool stating if the attributions should be normalised</li> <li><code>normalise_func</code>: a Callable that make a normalising transformation of the attributions</li> <li><code>nr_runs</code> (integer): the number of runs (for each input and explanation pair), default=100.</li> <li><code>subset_size</code> (integer): the size of subset, default=224.</li> <li><code>perturb_baseline</code> (string): indicates the type of baseline: \"mean\", \"random\", \"uniform\", \"black\" or \"white\", default=\"mean\".</li> <li><code>similarity_func</code> (callable): Similarity function applied to compare input and perturbed input, default=correlation_spearman.</li> <li><code>perturb_func</code> (callable): input perturbation function, default=baseline_replacement_by_indices.</li> <li><code>features_in_step</code> (integer): the size of the step, default=256.</li> <li><code>softmax</code> (boolean): indicates wheter to use softmax probabilities or logits in model prediction, default=True.</li> </ul> <p>\"call\": No parameters are used.</p> Source code in <code>autoexplainer/metrics.py</code> <pre><code>class FaithfulnessEstimateHandler(MetricHandler):\n\"\"\"\n    Metric handler for Faithfulness Estimate metric [(Alvarez-Melis et al., 2018)](https://arxiv.org/abs/1806.07538).\n\n    Computes the correlation between probability drops and attribution scores on various points.\n\n    Dictionary with parameters to override must be in the form:\n    ```\n    metric_parameters = {\"init\": &lt;dictionary with parameters used in metric's __init__&gt;,\n                       \"call\": &lt;dictionary with parameters used in metric's __call__&gt;}\n    ```\n\n    Parameters accepted in `metric_parameters`:\n\n    **\"init\"**:\n\n    * `abs`: a bool stating if absolute operation should be taken on the attributions\n    * `normalise`: a bool stating if the attributions should be normalised\n    * `normalise_func`: a Callable that make a normalising transformation of the attributions\n    * `nr_runs` (integer): the number of runs (for each input and explanation pair), default=100.\n    * `subset_size` (integer): the size of subset, default=224.\n    * `perturb_baseline` (string): indicates the type of baseline: \"mean\", \"random\", \"uniform\", \"black\" or \"white\", default=\"mean\".\n    * `similarity_func` (callable): Similarity function applied to compare input and perturbed input, default=correlation_spearman.\n    * `perturb_func` (callable): input perturbation function, default=baseline_replacement_by_indices.\n    * `features_in_step` (integer): the size of the step, default=256.\n    * `softmax` (boolean): indicates wheter to use softmax probabilities or logits in model prediction, default=True.\n\n    \"call\": No parameters are used.\n\n    \"\"\"\n\n    def __init__(\n        self, model: torch.nn.Module, data: torch.Tensor, targets: torch.Tensor, metric_parameters: Dict = None\n    ) -&gt; None:\n        self.metric_parameters = {}\n        self.metric_parameters[\"init\"] = self._infer_metric_parameters(model, data, targets)\n        self.metric_parameters[\"init\"] = self._add_hide_output_parameters_to_dict(self.metric_parameters[\"init\"])\n        if str(next(model.parameters()).device) == \"cpu\":\n            self.metric_parameters[\"call\"] = {}\n        else:\n            self.metric_parameters[\"call\"] = {\"device\": \"cuda\"}\n        if metric_parameters is not None:\n            update_dictionary(self.metric_parameters, metric_parameters)\n        self.metric = quantus.FaithfulnessEstimate(**self.metric_parameters[\"init\"])\n\n    def _infer_metric_parameters(self, model: torch.nn.Module, data: torch.Tensor, targets: torch.Tensor) -&gt; Dict:\n        parameters = {\n            \"normalise\": True,\n            \"features_in_step\": 256,\n            \"perturb_baseline\": \"black\",\n            \"softmax\": True,\n        }\n        return parameters\n</code></pre>"},{"location":"api_docs/#autoexplainer.metrics.IROFHandler","title":"<code>IROFHandler</code>","text":"<p>         Bases: <code>MetricHandler</code></p> <p>Metric handler for Iterative Removal Of Features metric (Rieger at el., 2020).</p> <p>Computes the area over the curve per class for sorted mean importances of feature segments (superpixels) as they are iteratively removed (and prediction scores are collected), averaged over several test samples.</p> <p>Dictionary with parameters to override must be in the form:</p> <pre><code>metric_parameters = {\"init\": &lt;dictionary with parameters used in metric's __init__&gt;,\n                   \"call\": &lt;dictionary with parameters used in metric's __call__&gt;}\n</code></pre> <p>Parameters accepted in <code>metric_parameters</code>: \"init\": - abs: a bool stating if absolute operation should be taken on the attributions - normalise: a bool stating if the attributions should be normalised - normalise_func: a Callable that make a normalising transformation of the attributions - segmentation_method (string): Image segmentation method: 'slic' or 'felzenszwalb', default=\"slic\" - perturb_baseline (string): indicates the type of baseline: \"mean\", \"random\", \"uniform\", \"black\" or \"white\", default=\"mean\" - perturb_func (callable): input perturbation function, default=baseline_replacement_by_indices - softmax (boolean): indicates wheter to use softmax probabilities or logits in model prediction</p> <p>\"call\": No parameters are used.</p> Source code in <code>autoexplainer/metrics.py</code> <pre><code>class IROFHandler(MetricHandler):\n\"\"\"\n    Metric handler for Iterative Removal Of Features metric [(Rieger at el., 2020)](https://arxiv.org/abs/2003.08747).\n\n    Computes the area over the curve per class for sorted mean importances of feature segments (superpixels)\n    as they are iteratively removed (and prediction scores are collected), averaged over several test samples.\n\n    Dictionary with parameters to override must be in the form:\n    ```\n    metric_parameters = {\"init\": &lt;dictionary with parameters used in metric's __init__&gt;,\n                       \"call\": &lt;dictionary with parameters used in metric's __call__&gt;}\n    ```\n    Parameters accepted in `metric_parameters`:\n    \"init\":\n    - abs: a bool stating if absolute operation should be taken on the attributions\n    - normalise: a bool stating if the attributions should be normalised\n    - normalise_func: a Callable that make a normalising transformation of the attributions\n    - segmentation_method (string): Image segmentation method: 'slic' or 'felzenszwalb', default=\"slic\"\n    - perturb_baseline (string): indicates the type of baseline: \"mean\", \"random\", \"uniform\", \"black\" or \"white\", default=\"mean\"\n    - perturb_func (callable): input perturbation function, default=baseline_replacement_by_indices\n    - softmax (boolean): indicates wheter to use softmax probabilities or logits in model prediction\n\n    \"call\": No parameters are used.\n\n    \"\"\"\n\n    def __init__(\n        self, model: torch.nn.Module, data: torch.Tensor, targets: torch.Tensor, metric_parameters: Dict = None\n    ) -&gt; None:\n        self.metric_parameters = {\"init\": self._infer_irof_parameters(model, data, targets), \"call\": {}}\n        self.metric_parameters[\"init\"] = self._add_hide_output_parameters_to_dict(self.metric_parameters[\"init\"])\n        if str(next(model.parameters()).device) != \"cpu\":\n            self.metric_parameters[\"call\"] = {\"device\": \"cuda\"}\n        if metric_parameters is not None:\n            update_dictionary(self.metric_parameters, metric_parameters)\n        self.metric = quantus.IterativeRemovalOfFeatures(**self.metric_parameters[\"init\"])\n\n    def _infer_irof_parameters(self, model: torch.nn.Module, data: torch.Tensor, targets: torch.Tensor) -&gt; Dict:\n        parameters = {\n            \"segmentation_method\": \"slic\",\n            \"perturb_baseline\": \"mean\",\n            \"softmax\": True,\n            \"return_aggregate\": False,\n        }\n        return parameters\n</code></pre>"},{"location":"api_docs/#autoexplainer.metrics.MetricHandler","title":"<code>MetricHandler</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Abstract class for metrics handlers.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>torch.nn.Module</code> <p>Model used for metrics' parameters inference.</p> required <code>data</code> <code>torch.Tensor</code> <p>Data used for metrics' parameters inference.</p> required <code>targets</code> <code>torch.Tensor</code> <p>Target used for metrics' parameters inference.</p> required <code>metric_parameters</code> <code>Dict</code> <p>Metric parameters to overwrite inferred parameters. Dictionary must be in the form:</p> <pre><code>                  ```\n                  metric_parameters = {\"init\": &lt;dictionary with parameters used in metric's __init__&gt;,\n                                       \"call\": &lt;dictionary with parameters used in metric's __call__&gt;}\n                      ```\n</code></pre> <code>None</code> <p>Attributes:</p> Name Type Description <code>metric</code> <code>quantus.Metric</code> <p>Attribute that stores created metric object after determinig its parameters.</p> <code>metric_parameters</code> <code>Dict</code> <p>Dictionary with parameters used for this metric.</p> Source code in <code>autoexplainer/metrics.py</code> <pre><code>class MetricHandler(ABC):\n\"\"\"\n    Abstract class for metrics handlers.\n\n\n\n    Args:\n        model (torch.nn.Module): Model used for metrics' parameters inference.\n        data (torch.Tensor): Data used for metrics' parameters inference.\n        targets (torch.Tensor): Target used for metrics' parameters inference.\n        metric_parameters (Dict): Metric parameters to overwrite inferred parameters. Dictionary must be in the form:\n\n                                  ```\n                                  metric_parameters = {\"init\": &lt;dictionary with parameters used in metric's __init__&gt;,\n                                                       \"call\": &lt;dictionary with parameters used in metric's __call__&gt;}\n                                      ```\n\n\n    Attributes:\n        metric (quantus.Metric): Attribute that stores created metric object after determinig its parameters.\n        metric_parameters (Dict): Dictionary with parameters used for this metric.\n\n    \"\"\"\n\n    metric: quantus.Metric = NotImplemented  # type: ignore[no-any-unimported]\n    metric_parameters: Dict = None\n\n    @abstractmethod\n    def __init__(\n        self, model: torch.nn.Module, data: torch.Tensor, targets: torch.Tensor, metric_parameters: Dict = None\n    ) -&gt; None:\n        pass\n\n    def compute_metric_values(\n        self,\n        model: torch.nn.Module,\n        data: torch.Tensor,\n        targets: torch.Tensor,\n        attributions: torch.Tensor = None,\n        explanation_func: Callable = None,\n    ) -&gt; np.ndarray:\n\"\"\"\n        Computes metric values for given model, dataset, and explanation function.\n        Args:\n            model:\n            data:\n            targets:\n            attributions:\n            explanation_func:\n\n        Returns:\n            NumPy array with metric values for each given image.\n        \"\"\"\n        x_batch = deepcopy(data.cpu().detach().numpy())\n        y_batch = deepcopy(targets.cpu().detach().numpy())\n        if attributions is not None:\n            a_batch = deepcopy(attributions.cpu().detach().numpy())\n        else:\n            a_batch = None\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", category=UserWarning)\n            result_list: List[float] = self.metric(\n                model=model,\n                x_batch=x_batch,\n                y_batch=y_batch,\n                a_batch=a_batch,\n                explain_func=explanation_func,\n                **self.metric_parameters[\"call\"],\n            )\n        result: np.ndarray = np.array(result_list)\n        return result\n\n    def get_parameters(self) -&gt; Dict:\n        return self.metric_parameters\n\n    def _add_hide_output_parameters_to_dict(self, metric_parameters: Dict) -&gt; Dict:\n        metric_parameters[\"disable_warnings\"] = True\n        metric_parameters[\"display_progressbar\"] = False\n        return metric_parameters\n</code></pre>"},{"location":"api_docs/#autoexplainer.metrics.MetricHandler.compute_metric_values","title":"<code>compute_metric_values(model: torch.nn.Module, data: torch.Tensor, targets: torch.Tensor, attributions: torch.Tensor = None, explanation_func: Callable = None) -&gt; np.ndarray</code>","text":"<p>Computes metric values for given model, dataset, and explanation function.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>torch.nn.Module</code> required <code>data</code> <code>torch.Tensor</code> required <code>targets</code> <code>torch.Tensor</code> required <code>attributions</code> <code>torch.Tensor</code> <code>None</code> <code>explanation_func</code> <code>Callable</code> <code>None</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>NumPy array with metric values for each given image.</p> Source code in <code>autoexplainer/metrics.py</code> <pre><code>def compute_metric_values(\n    self,\n    model: torch.nn.Module,\n    data: torch.Tensor,\n    targets: torch.Tensor,\n    attributions: torch.Tensor = None,\n    explanation_func: Callable = None,\n) -&gt; np.ndarray:\n\"\"\"\n    Computes metric values for given model, dataset, and explanation function.\n    Args:\n        model:\n        data:\n        targets:\n        attributions:\n        explanation_func:\n\n    Returns:\n        NumPy array with metric values for each given image.\n    \"\"\"\n    x_batch = deepcopy(data.cpu().detach().numpy())\n    y_batch = deepcopy(targets.cpu().detach().numpy())\n    if attributions is not None:\n        a_batch = deepcopy(attributions.cpu().detach().numpy())\n    else:\n        a_batch = None\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=UserWarning)\n        result_list: List[float] = self.metric(\n            model=model,\n            x_batch=x_batch,\n            y_batch=y_batch,\n            a_batch=a_batch,\n            explain_func=explanation_func,\n            **self.metric_parameters[\"call\"],\n        )\n    result: np.ndarray = np.array(result_list)\n    return result\n</code></pre>"},{"location":"api_docs/#autoexplainer.metrics.SparsenessHandler","title":"<code>SparsenessHandler</code>","text":"<p>         Bases: <code>MetricHandler</code></p> <p>Metric Handler for Sparseness metric (Chalasani et al., 2020).</p> <p>Uses the Gini Index for measuring, if only highly attributed features are truly predictive of the model output.</p> <p>Dictionary with parameters to override must be in the form:</p> <pre><code>metric_parameters = {\"init\": &lt;dictionary with parameters used in metric's __init__&gt;,\n                   \"call\": &lt;dictionary with parameters used in metric's __call__&gt;}\n</code></pre> <p>Parameters accepted in <code>metric_parameters</code>:</p> <p>\"init\":</p> <ul> <li>abs: a bool stating if absolute operation should be taken on the attributions</li> <li>normalise: a bool stating if the attributions should be normalised</li> <li>normalise_func: a Callable that make a normalising transformation of the attributions</li> </ul> <p>\"call\": No parameters are used.</p> Source code in <code>autoexplainer/metrics.py</code> <pre><code>class SparsenessHandler(MetricHandler):\n\"\"\"\n    Metric Handler for Sparseness metric [(Chalasani et al., 2020)](https://arxiv.org/abs/1810.06583).\n\n    Uses the Gini Index for measuring, if only highly attributed features are truly predictive of the model output.\n\n\n    Dictionary with parameters to override must be in the form:\n    ```\n    metric_parameters = {\"init\": &lt;dictionary with parameters used in metric's __init__&gt;,\n                       \"call\": &lt;dictionary with parameters used in metric's __call__&gt;}\n    ```\n\n    Parameters accepted in `metric_parameters`:\n\n    \"init\":\n\n    - abs: a bool stating if absolute operation should be taken on the attributions\n    - normalise: a bool stating if the attributions should be normalised\n    - normalise_func: a Callable that make a normalising transformation of the attributions\n\n    \"call\": No parameters are used.\n\n\n    \"\"\"\n\n    def __init__(\n        self, model: torch.nn.Module, data: torch.Tensor, targets: torch.Tensor, metric_parameters: Dict = None\n    ) -&gt; None:\n        self.metric_parameters = {\"init\": self._infer_sparsness_parameters(model, data, targets), \"call\": {}}\n        self.metric_parameters[\"init\"] = self._add_hide_output_parameters_to_dict(self.metric_parameters[\"init\"])\n        if str(next(model.parameters()).device) != \"cpu\":\n            self.metric_parameters[\"call\"] = {\"device\": \"cuda\"}\n        if metric_parameters is not None:\n            update_dictionary(self.metric_parameters, metric_parameters)\n        self.metric = quantus.Sparseness(**self.metric_parameters[\"init\"])\n\n    def _infer_sparsness_parameters(self, model: torch.nn.Module, data: torch.Tensor, targets: torch.Tensor) -&gt; Dict:\n        return {}\n</code></pre>"},{"location":"api_docs/#explanation-methods-handlers","title":"Explanation methods handlers","text":""},{"location":"api_docs/#autoexplainer.explanations.explanation_handlers.BestExplanation","title":"<code>BestExplanation</code>","text":"<p>Class for an object that wraps the best explanation method selected during the evaluation process.</p> <p>Attributes:</p> Name Type Description <code>attributions</code> <code>torch.Tensor</code> <p>Attributions computed during evaluation using this explanation method only.</p> <code>explanation_function</code> <code>torch.Tensor</code> <p>Function that computes attributions for the provided model and data.</p> <code>name</code> <code>str</code> <p>Name of this explanation method.</p> <code>parameters</code> <code>Dict</code> <p>Parameters used in this explanation method.</p> <code>metric_handlers</code> <code>Dict</code> <p>Dictionary with metric handlers that this explanation method was evaluated with.</p> <code>aggregation_parameters</code> <code>Dict</code> <p>Parameter that were used during aggregation of metric values.</p> Source code in <code>autoexplainer/explanations/explanation_handlers.py</code> <pre><code>class BestExplanation:\n\"\"\"\n    Class for an object that wraps the best explanation method selected during the evaluation process.\n\n    Attributes:\n         attributions (torch.Tensor): Attributions computed during evaluation using this explanation method only.\n         explanation_function (torch.Tensor): Function that computes attributions for the provided model and data.\n         name (str): Name of this explanation method.\n         parameters (Dict): Parameters used in this explanation method.\n         metric_handlers (Dict): Dictionary with metric handlers that this explanation method was evaluated with.\n         aggregation_parameters (Dict): Parameter that were used during aggregation of metric values.\n    \"\"\"\n\n    attributions: torch.Tensor = None\n    explanation_function: Callable = None\n    name: str = None\n    parameters: Dict = None\n    metric_handlers: Dict = None\n    aggregation_parameters: Dict = None\n\n    def __init__(\n        self,\n        attributions: torch.Tensor,\n        explanation_function: Callable,\n        explanation_name: str,\n        explanation_function_parameters: Dict,\n        metric_handlers: Dict,\n        aggregation_parameters: Dict,\n    ) -&gt; None:\n        self.attributions = attributions\n        self.explanation_function = explanation_function\n        self.name = explanation_name\n        self.parameters = explanation_function_parameters\n        self.metric_handlers = metric_handlers\n        self.aggregation_parameters = aggregation_parameters\n\n    def explain(self, model: torch.nn.Module, data: torch.Tensor, targets: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"\n        Compute new attributions.\n        Args:\n            model (torch.nn.Module): CNN neural network to be explained.\n            data (torch.Tensor): Data for which attributions will be computed. shape: (N, C, H, W).\n            targets (torch.Tensor): Labels for provided data. Encoded as integer vector with shape (N,).\n\n        Returns:\n            attributions (torch.Tensor)\n\n        \"\"\"\n        self._check_model_and_data(model, data, targets)\n        print(f\"Computing attributions using {EXPLANATION_NAME_SHORT_TO_LONG[self.name]} method.\")\n        print(\"This may take a while, depending on the number of samples to be explained.\")\n        model = fix_relus_in_model(model)\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", category=UserWarning)\n            attributions_list = []\n            for x, y in tqdm.tqdm(zip(data, targets), total=len(data), desc=\"Calculating attributions\"):\n                attr = self.explanation_function(\n                    model=model,\n                    inputs=x.reshape(1, *x.shape).to(next(model.parameters()).device),\n                    targets=y.to(next(model.parameters()).device),\n                )\n                attributions_list.append(torch.tensor(attr)[0])\n        all_attributions = torch.stack(attributions_list, dim=0)\n        print(\"Finished.\")\n        return all_attributions\n\n    def _check_model_and_data(self, model: torch.nn.Module, data: torch.Tensor, targets: torch.Tensor) -&gt; None:\n        if not isinstance(model, torch.nn.Module):\n            raise TypeError(\"Model must be of type torch.nn.Module.\")  # noqa: TC003\n        if not isinstance(data, torch.Tensor):\n            raise TypeError(\"Data must be of type torch.Tensor.\")  # noqa: TC003\n        if len(data.shape) != 4:\n            raise ValueError(\"Data must be of shape (N, C, H, W).\")  # noqa: TC003\n        if not isinstance(targets, torch.Tensor):\n            raise TypeError(\"Targets must be of type torch.Tensor.\")  # noqa: TC003\n        if len(targets.shape) != 1:\n            raise ValueError(\"Targets must be of shape (N,).\")  # noqa: TC003\n        if data.shape[0] != targets.shape[0]:\n            raise ValueError(\"Data and targets must have the same number of observations.\")  # noqa: TC003\n\n    def evaluate(\n        self,\n        model: torch.nn.Module,\n        data: torch.Tensor,\n        targets: torch.Tensor,\n        attributions: torch.Tensor = None,\n        aggregate: bool = False,\n    ) -&gt; Dict:\n\"\"\"\n        Evaluate the selected best explanation method again on new data.\n        Args:\n            model (torch.nn.Module): Convolutional neural network to be explained.\n            data (torch.Tensor): Data that will be used for the evaluation of the explanation method. shape: (N, C, H, W).\n            targets (torch.Tensor): Labels for provided data. Encoded as integer vector with shape (N,).\n            attributions (torch.Tensor, optional): Attributions for this data that were previously computed, to skip computing them once more.\n            aggregate (bool, optional): Indicates whether results should be aggregated (in the same manner as in AutoExplainer).\n\n        Returns:\n            results (Dict): Results of evaluation.\n        \"\"\"\n        self._check_model_and_data(model, data, targets)\n        if attributions is not None:\n            self._check_attributions(data, attributions)\n        self._check_is_bool(aggregate, \"aggregate\")\n        print(\n            f\"Evaluating explanation method {EXPLANATION_NAME_SHORT_TO_LONG[self.name]} using {len(self.metric_handlers)} metrics.\"\n        )\n        print(f\"\\tMetrics: {', '.join([METRIC_NAME_SHORT_TO_LONG[x] for x in list(self.metric_handlers.keys())])}\")\n        print(\"This may take a long time, depending on the number of samples and metrics.\")\n        model = fix_relus_in_model(model)\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", category=UserWarning)\n            raw_results = {}\n            pbar = tqdm.tqdm(self.metric_handlers.items(), total=len(self.metric_handlers), desc=\"Evaluating\")\n            for metric_name, metric_handler in pbar:\n                raw_results[metric_name] = metric_handler.compute_metric_values(\n                    model=model,\n                    data=data,\n                    targets=targets,\n                    attributions=attributions,\n                    explanation_func=self.explanation_function,\n                )\n        if not aggregate:\n            return raw_results\n        else:\n            raw_results = {self.name: raw_results}\n            first_aggregation_results = self.aggregation_parameters[\"first_stage_aggregation_function\"](raw_results)\n            return first_aggregation_results[self.name]\n\n    def _check_attributions(self, data: torch.Tensor, attributions: torch.Tensor) -&gt; None:\n        if not isinstance(attributions, torch.Tensor):\n            raise TypeError(\"Attributions must be of type torch.Tensor.\")  # noqa: TC003\n        if len(attributions.shape) != 4:\n            raise ValueError(\"Attributions must be of shape (N, C, H, W).\")  # noqa: TC003\n        if data.shape[0] != attributions.shape[0]:\n            raise ValueError(\"Data and targets must have the same number of observations.\")  # noqa: TC003\n        if data.shape[2:] != attributions.shape[2:]:\n            raise ValueError(\"Data and attributions must have the same image shape.\")  # noqa: TC003\n\n    def _check_is_bool(self, value: bool, value_name: str) -&gt; None:\n        if not isinstance(value, bool):\n            raise TypeError(f\"Value  of {value_name} must be of type bool.\")  # noqa: TC003\n</code></pre>"},{"location":"api_docs/#autoexplainer.explanations.explanation_handlers.BestExplanation.evaluate","title":"<code>evaluate(model: torch.nn.Module, data: torch.Tensor, targets: torch.Tensor, attributions: torch.Tensor = None, aggregate: bool = False) -&gt; Dict</code>","text":"<p>Evaluate the selected best explanation method again on new data.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>torch.nn.Module</code> <p>Convolutional neural network to be explained.</p> required <code>data</code> <code>torch.Tensor</code> <p>Data that will be used for the evaluation of the explanation method. shape: (N, C, H, W).</p> required <code>targets</code> <code>torch.Tensor</code> <p>Labels for provided data. Encoded as integer vector with shape (N,).</p> required <code>attributions</code> <code>torch.Tensor</code> <p>Attributions for this data that were previously computed, to skip computing them once more.</p> <code>None</code> <code>aggregate</code> <code>bool</code> <p>Indicates whether results should be aggregated (in the same manner as in AutoExplainer).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>results</code> <code>Dict</code> <p>Results of evaluation.</p> Source code in <code>autoexplainer/explanations/explanation_handlers.py</code> <pre><code>def evaluate(\n    self,\n    model: torch.nn.Module,\n    data: torch.Tensor,\n    targets: torch.Tensor,\n    attributions: torch.Tensor = None,\n    aggregate: bool = False,\n) -&gt; Dict:\n\"\"\"\n    Evaluate the selected best explanation method again on new data.\n    Args:\n        model (torch.nn.Module): Convolutional neural network to be explained.\n        data (torch.Tensor): Data that will be used for the evaluation of the explanation method. shape: (N, C, H, W).\n        targets (torch.Tensor): Labels for provided data. Encoded as integer vector with shape (N,).\n        attributions (torch.Tensor, optional): Attributions for this data that were previously computed, to skip computing them once more.\n        aggregate (bool, optional): Indicates whether results should be aggregated (in the same manner as in AutoExplainer).\n\n    Returns:\n        results (Dict): Results of evaluation.\n    \"\"\"\n    self._check_model_and_data(model, data, targets)\n    if attributions is not None:\n        self._check_attributions(data, attributions)\n    self._check_is_bool(aggregate, \"aggregate\")\n    print(\n        f\"Evaluating explanation method {EXPLANATION_NAME_SHORT_TO_LONG[self.name]} using {len(self.metric_handlers)} metrics.\"\n    )\n    print(f\"\\tMetrics: {', '.join([METRIC_NAME_SHORT_TO_LONG[x] for x in list(self.metric_handlers.keys())])}\")\n    print(\"This may take a long time, depending on the number of samples and metrics.\")\n    model = fix_relus_in_model(model)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=UserWarning)\n        raw_results = {}\n        pbar = tqdm.tqdm(self.metric_handlers.items(), total=len(self.metric_handlers), desc=\"Evaluating\")\n        for metric_name, metric_handler in pbar:\n            raw_results[metric_name] = metric_handler.compute_metric_values(\n                model=model,\n                data=data,\n                targets=targets,\n                attributions=attributions,\n                explanation_func=self.explanation_function,\n            )\n    if not aggregate:\n        return raw_results\n    else:\n        raw_results = {self.name: raw_results}\n        first_aggregation_results = self.aggregation_parameters[\"first_stage_aggregation_function\"](raw_results)\n        return first_aggregation_results[self.name]\n</code></pre>"},{"location":"api_docs/#autoexplainer.explanations.explanation_handlers.BestExplanation.explain","title":"<code>explain(model: torch.nn.Module, data: torch.Tensor, targets: torch.Tensor) -&gt; torch.Tensor</code>","text":"<p>Compute new attributions.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>torch.nn.Module</code> <p>CNN neural network to be explained.</p> required <code>data</code> <code>torch.Tensor</code> <p>Data for which attributions will be computed. shape: (N, C, H, W).</p> required <code>targets</code> <code>torch.Tensor</code> <p>Labels for provided data. Encoded as integer vector with shape (N,).</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>attributions (torch.Tensor)</p> Source code in <code>autoexplainer/explanations/explanation_handlers.py</code> <pre><code>def explain(self, model: torch.nn.Module, data: torch.Tensor, targets: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"\n    Compute new attributions.\n    Args:\n        model (torch.nn.Module): CNN neural network to be explained.\n        data (torch.Tensor): Data for which attributions will be computed. shape: (N, C, H, W).\n        targets (torch.Tensor): Labels for provided data. Encoded as integer vector with shape (N,).\n\n    Returns:\n        attributions (torch.Tensor)\n\n    \"\"\"\n    self._check_model_and_data(model, data, targets)\n    print(f\"Computing attributions using {EXPLANATION_NAME_SHORT_TO_LONG[self.name]} method.\")\n    print(\"This may take a while, depending on the number of samples to be explained.\")\n    model = fix_relus_in_model(model)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=UserWarning)\n        attributions_list = []\n        for x, y in tqdm.tqdm(zip(data, targets), total=len(data), desc=\"Calculating attributions\"):\n            attr = self.explanation_function(\n                model=model,\n                inputs=x.reshape(1, *x.shape).to(next(model.parameters()).device),\n                targets=y.to(next(model.parameters()).device),\n            )\n            attributions_list.append(torch.tensor(attr)[0])\n    all_attributions = torch.stack(attributions_list, dim=0)\n    print(\"Finished.\")\n    return all_attributions\n</code></pre>"},{"location":"api_docs/#autoexplainer.explanations.explanation_handlers.ExplanationHandler","title":"<code>ExplanationHandler</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Abstract class for explanation methods handlers. Handlers manage explanation methods: they read and adapt parameters for given model and data. They also create explanation functions that may be used by the user or can be passed to metric handlers.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>torch.nn.Module</code> <p>Model used for methods' parameter adaptation.</p> required <code>data</code> <code>torch.Tensor</code> <p>Data used for method's parameters adaptation. Tensor with shape (N, C, H, W)</p> required <code>targets</code> <code>torch.Tensor</code> <p>Target used for method's parameters inference - integer vector with shape (N,)</p> required <code>explanation_parameters</code> <code>Dict</code> <p>Explanation method parameters to be overwritten.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>explanation_function</code> <code>Callable</code> <p>Explanation method as function ready to be used with already set parameters.</p> <code>explanation_parameters</code> <code>Dict</code> <p>Parameters chosen for given explanation method.</p> <code>attributions</code> <code>torch.Tensor</code> <p>Computed attributions, only most recent.</p> Source code in <code>autoexplainer/explanations/explanation_handlers.py</code> <pre><code>class ExplanationHandler(ABC):\n\"\"\"\n    Abstract class for explanation methods handlers. Handlers manage explanation methods: they read and adapt parameters for\n    given model and data. They also create explanation functions that may be used by the user or can be passed to metric handlers.\n\n    Parameters:\n        model (torch.nn.Module): Model used for methods' parameter adaptation.\n        data (torch.Tensor): Data used for method's parameters adaptation. Tensor with shape (N, C, H, W)\n        targets (torch.Tensor): Target used for method's parameters inference - integer vector with shape (N,)\n        explanation_parameters (Dict): Explanation method parameters to be overwritten.\n\n    Attributes:\n        explanation_function (Callable): Explanation method as function ready to be used with already set parameters.\n        explanation_parameters (Dict): Parameters chosen for given explanation method.\n        attributions (torch.Tensor): Computed attributions, only most recent.\n\n    \"\"\"\n\n    explanation_function: Callable = NotImplemented\n    explanation_parameters: Dict = None\n    attributions: torch.Tensor = None\n\n    @abstractmethod\n    def __init__(\n        self, model: torch.nn.Module, data: torch.Tensor, targets: torch.Tensor, explanation_parameters: Dict = None\n    ) -&gt; None:\n        pass\n\n    def explain(self, model: torch.nn.Module, data: torch.Tensor, targets: torch.Tensor) -&gt; torch.Tensor:\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", category=UserWarning)\n            self.attributions = torch.tensor(self.explanation_function(model, data, targets)).to(\n                next(model.parameters()).device\n            )\n        return self.attributions\n\n    def get_explanation_function(self) -&gt; Callable:\n\"\"\"Return function that can be run by Quantus metrics.\"\"\"\n        return self.explanation_function\n\n    def get_parameters(self) -&gt; Dict:\n        return self.explanation_parameters\n\n    def _create_mask_function(self, mask_parameters: Dict) -&gt; Callable:\n        mask_function_name: str = mask_parameters.get(\"mask_function_name\")\n        del mask_parameters[\"mask_function_name\"]\n        return partial(batch_segmentation, mask_function_name=mask_function_name, **mask_parameters)\n</code></pre>"},{"location":"api_docs/#autoexplainer.explanations.explanation_handlers.ExplanationHandler.get_explanation_function","title":"<code>get_explanation_function() -&gt; Callable</code>","text":"<p>Return function that can be run by Quantus metrics.</p> Source code in <code>autoexplainer/explanations/explanation_handlers.py</code> <pre><code>def get_explanation_function(self) -&gt; Callable:\n\"\"\"Return function that can be run by Quantus metrics.\"\"\"\n    return self.explanation_function\n</code></pre>"},{"location":"api_docs/#autoexplainer.explanations.explanation_handlers.GradCamHandler","title":"<code>GradCamHandler</code>","text":"<p>         Bases: <code>ExplanationHandler</code></p> <p>Handler for GradCam explanation method. Uses captum implementation of GradCam.</p> <p>By default, the last convolutional layer is chosen as a parameter for GradCam.</p> <p>To overwrite default parameters, passed dictionary must be in the form:</p> <pre><code>explanation_parameters = {\"explanation_parameters\":{ &lt;parameters accepted by GradCam in Captum&gt; }}\n</code></pre> Source code in <code>autoexplainer/explanations/explanation_handlers.py</code> <pre><code>class GradCamHandler(ExplanationHandler):\n\"\"\"\n    Handler for GradCam explanation method. Uses captum implementation of GradCam.\n\n    By default, the last convolutional layer is chosen as a parameter for GradCam.\n\n    To overwrite default parameters, passed dictionary must be in the form:\n    ```\n    explanation_parameters = {\"explanation_parameters\":{ &lt;parameters accepted by GradCam in Captum&gt; }}\n    ```\n    \"\"\"\n\n    def __init__(\n        self, model: torch.nn.Module, data: torch.Tensor, targets: torch.Tensor, explanation_parameters: Dict = None\n    ) -&gt; None:\n        self.explanation_parameters = {}\n        self.explanation_parameters[\"explanation_parameters\"] = self._infer_grad_cam_parameters(model, data)\n        if explanation_parameters is not None:\n            update_dictionary(self.explanation_parameters, explanation_parameters)\n        if self.explanation_parameters[\"explanation_parameters\"].get(\"selected_layer\") is None:\n            raise ValueError(\"Unrecognized model, you need to pass selected layer name for GradCam.\")  # noqa: TC003\n        self.explanation_function = partial(grad_cam, **self.explanation_parameters[\"explanation_parameters\"])\n\n    def _infer_grad_cam_parameters(self, model: torch.nn.Module, data: torch.Tensor) -&gt; Dict:\n        parameters = {}\n        layer_name = self._get_last_conv_layer_name(model)\n        parameters[\"selected_layer\"] = layer_name\n        parameters[\"relu_attributions\"] = True\n        return parameters\n\n    def _get_last_conv_layer_name(self, model: torch.nn.Module) -&gt; Union[str, None]:\n        last_conv_layer_name = None\n        for name, layer in model.named_modules():\n            if isinstance(layer, torch.nn.Conv2d):\n                last_conv_layer_name = name\n        if last_conv_layer_name:\n            return last_conv_layer_name\n        return None\n</code></pre>"},{"location":"api_docs/#autoexplainer.explanations.explanation_handlers.IntegratedGradients","title":"<code>IntegratedGradients</code>","text":"<p>         Bases: <code>ExplanationHandler</code></p> <p>Handler for Integrated Gradients explanation method. Uses implementation in Quantus library.</p> <p>To overwrite default parameters, passed dictionary must be in the form:</p> <pre><code>explanation_parameters = {\"explanation_parameters\":{ &lt;parameters&gt; }}\n</code></pre> <p>Integrated Gradients method accepts paramteters:</p> <ul> <li><code>normalise</code> (Bool) - Normalize attribution values. default=False</li> <li><code>abs</code> (bool) - Return absolute values of attribtuion. default=False</li> <li>'pos_only` (bool) - Clip negative values of attribution to 0.0. default=False</li> <li><code>neg_only</code> (bool) - Clip positive values of attribution to 0.0. default=False</li> </ul> Source code in <code>autoexplainer/explanations/explanation_handlers.py</code> <pre><code>class IntegratedGradients(ExplanationHandler):\n\"\"\"\n    Handler for Integrated Gradients explanation method. Uses implementation in Quantus library.\n\n    To overwrite default parameters, passed dictionary must be in the form:\n    ```\n    explanation_parameters = {\"explanation_parameters\":{ &lt;parameters&gt; }}\n    ```\n\n    Integrated Gradients method accepts paramteters:\n\n    - `normalise` (Bool) - Normalize attribution values. default=False\n    - `abs` (bool) - Return absolute values of attribtuion. default=False\n    - 'pos_only` (bool) - Clip negative values of attribution to 0.0. default=False\n    - `neg_only` (bool) - Clip positive values of attribution to 0.0. default=False\n\n    \"\"\"\n\n    # TODO: use original captum implementation instead Quantus'. Quantus implementation have hardcoded parameters\n    def __init__(\n        self, model: torch.nn.Module, data: torch.Tensor, targets: torch.Tensor, explanation_parameters: Dict = None\n    ) -&gt; None:\n        self.explanation_parameters = {}\n\n        self.explanation_parameters[\"explanation_parameters\"] = self._infer_ig_parameters(model, data)\n        if explanation_parameters is not None:\n            update_dictionary(self.explanation_parameters, explanation_parameters)\n        self._map_baseline_function_name_to_function()\n        self.explanation_function = partial(\n            integrated_gradients_explanation,\n            **self.explanation_parameters[\"explanation_parameters\"],\n        )\n\n    def _infer_ig_parameters(self, model: torch.nn.Module, data: torch.Tensor) -&gt; Dict:\n        parameters = {\"baseline_function_name\": \"black\", \"n_steps\": 20}\n        return parameters\n\n    def _map_baseline_function_name_to_function(self) -&gt; None:\n        self.explanation_parameters[\"explanation_parameters\"][\"baseline_function\"] = BASELINE_FUNCTIONS[\n            self.explanation_parameters[\"explanation_parameters\"][\"baseline_function_name\"]\n        ]\n</code></pre>"},{"location":"api_docs/#autoexplainer.explanations.explanation_handlers.KernelShapHandler","title":"<code>KernelShapHandler</code>","text":"<p>         Bases: <code>ExplanationHandler</code></p> <p>Handler for Kernel Shap explanation. Uses captum implementation of Kernel Shap. Accepts parameters with form:</p> <p>To overwrite default parameters, passed dictionary must be in the form:</p> <pre><code>explanation_parameters = {\"mask_parameters\": { \"mask_function_name\":&lt;str&gt;, &lt;other parameters for chosen mask function&gt; },\n                   \"explanation_parameters\":{ \"baseline_function_name\":&lt;str&gt;, &lt;parameters accepted by KernelShap in Captum&gt; }}\n</code></pre> Source code in <code>autoexplainer/explanations/explanation_handlers.py</code> <pre><code>class KernelShapHandler(ExplanationHandler):\n\"\"\"\n    Handler for Kernel Shap explanation. Uses captum implementation of Kernel Shap. Accepts parameters with form:\n\n    To overwrite default parameters, passed dictionary must be in the form:\n    ```\n    explanation_parameters = {\"mask_parameters\": { \"mask_function_name\":&lt;str&gt;, &lt;other parameters for chosen mask function&gt; },\n                       \"explanation_parameters\":{ \"baseline_function_name\":&lt;str&gt;, &lt;parameters accepted by KernelShap in Captum&gt; }}\n    ```\n\n    \"\"\"\n\n    def __init__(\n        self, model: torch.nn.Module, data: torch.Tensor, targets: torch.Tensor, explanation_parameters: Dict = None\n    ) -&gt; None:\n        self.explanation_parameters = {}\n\n        self.explanation_parameters[\"mask_parameters\"] = self._infer_mask_parameters(data)\n        self.explanation_parameters[\"explanation_parameters\"] = self._infer_kernel_shap_parameters(data)\n        if explanation_parameters is not None:\n            update_dictionary(self.explanation_parameters, explanation_parameters)\n        self._set_baseline_function()\n        mask_function = self._create_mask_function(self.explanation_parameters[\"mask_parameters\"])\n        self.explanation_function = partial(\n            shap_explanation, mask_function=mask_function, **self.explanation_parameters[\"explanation_parameters\"]\n        )\n\n    # this method probably will be moved to the super class\n    def _infer_mask_parameters(self, data: torch.Tensor) -&gt; Dict:\n        parameters = {}\n        parameters[\"mask_function_name\"] = \"slic\"\n        parameters[\"n_segments\"] = 50\n        return parameters\n\n    def _infer_kernel_shap_parameters(self, data: torch.Tensor) -&gt; Dict:\n        parameters = {}\n        parameters[\"n_samples\"] = 50\n        parameters[\"baseline_function_name\"] = \"black\"\n        return parameters\n\n    def _set_baseline_function(self) -&gt; None:\n        self.explanation_parameters[\"explanation_parameters\"][\"baseline_function\"] = BASELINE_FUNCTIONS[\n            self.explanation_parameters[\"explanation_parameters\"][\"baseline_function_name\"]\n        ]\n</code></pre>"},{"location":"api_docs/#autoexplainer.explanations.explanation_handlers.SaliencyHandler","title":"<code>SaliencyHandler</code>","text":"<p>         Bases: <code>ExplanationHandler</code></p> <p>Handler for Saliency explanation method. Uses implementation in Quantus library.</p> <p>To overwrite default parameters, passed dictionary must be in the form:</p> <pre><code>explanation_parameters = {\"explanation_parameters\":{ &lt;parameters&gt; }}\n</code></pre> <p>Saliency method accepts paramteters:</p> <ul> <li><code>normalise</code> (Bool) - Normalize attribution values. default=False</li> <li><code>abs</code> (bool) - Return absolute values of attribtuion. default=False</li> <li>'pos_only` (bool) - Clip negative values of attribution to 0.0. default=False</li> <li><code>neg_only</code> (bool) - Clip positive values of attribution to 0.0. default=False</li> </ul> Source code in <code>autoexplainer/explanations/explanation_handlers.py</code> <pre><code>class SaliencyHandler(ExplanationHandler):\n\"\"\"\n    Handler for Saliency explanation method. Uses implementation in Quantus library.\n\n    To overwrite default parameters, passed dictionary must be in the form:\n    ```\n    explanation_parameters = {\"explanation_parameters\":{ &lt;parameters&gt; }}\n    ```\n\n    Saliency method accepts paramteters:\n\n    - `normalise` (Bool) - Normalize attribution values. default=False\n    - `abs` (bool) - Return absolute values of attribtuion. default=False\n    - 'pos_only` (bool) - Clip negative values of attribution to 0.0. default=False\n    - `neg_only` (bool) - Clip positive values of attribution to 0.0. default=False\n\n    \"\"\"\n\n    def __init__(\n        self, model: torch.nn.Module, data: torch.Tensor, targets: torch.Tensor, explanation_parameters: Dict = None\n    ) -&gt; None:\n        self.explanation_parameters = {}\n\n        self.explanation_parameters[\"explanation_parameters\"] = self._infer_saliency_parameters(model, data)\n        if explanation_parameters is not None:\n            update_dictionary(self.explanation_parameters, explanation_parameters)\n        self.explanation_function = partial(\n            saliency_explanation,\n            **self.explanation_parameters[\"explanation_parameters\"],\n        )\n\n    def _infer_saliency_parameters(self, model: torch.nn.Module, data: torch.Tensor) -&gt; Dict:\n        parameters = {\"abs\": True}\n        return parameters\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"Unreleased","text":""},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Fixed <code>second_stage_aggregation_weighted_mean</code>.</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Refactoring due to name change of package to <code>autoexplain</code>.</li> </ul>"},{"location":"changelog/#003-2022-12-06","title":"0.0.3 - 2022-12-06","text":""},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>[dummy] set 3.9 python-env with cache for draft release</li> </ul>"},{"location":"changelog/#002-2022-12-06","title":"0.0.2 - 2022-12-06","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>[placeholder] Add release function and many more.</li> </ul>"},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>[placeholder] GPU is working.</li> </ul>"}]}